{"forum": "rALA0Xo6yNJ", "review_ratings": ["7: Good paper, accept", "8: Top 50% of accepted papers, clear accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept"], "decision": "Oral", "content": {"title": "Learning to Reach Goals via Iterated Supervised Learning", "authorids": ["~Dibya_Ghosh1", "~Abhishek_Gupta1", "~Ashwin_Reddy1", "~Justin_Fu1", "~Coline_Manon_Devin1", "~Benjamin_Eysenbach1", "~Sergey_Levine1"], "authors": ["Dibya Ghosh", "Abhishek Gupta", "Ashwin Reddy", "Justin Fu", "Coline Manon Devin", "Benjamin Eysenbach", "Sergey Levine"], "keywords": ["goal reaching", "reinforcement learning", "behavior cloning", "goal-conditioned RL"], "abstract": "Current reinforcement learning (RL) algorithms can be brittle and difficult to use, especially when learning goal-reaching behaviors from sparse rewards. Although supervised imitation learning provides a simple and stable alternative, it requires access to demonstrations from a human supervisor. In this paper, we study RL algorithms that use imitation learning to acquire goal reaching policies from scratch, without the need for expert demonstrations or a value function. In lieu of demonstrations, we leverage the property that any trajectory is a successful demonstration for reaching the final state in that same trajectory. We propose a simple algorithm in which an agent continually relabels and imitates the trajectories it generates to progressively learn goal-reaching behaviors from scratch. Each iteration, the agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along these trajectories under the goal that was actually reached, so as to improve the policy. We formally show that this iterated supervised learning procedure optimizes a bound on the RL objective, derive performance bounds of the learned policy, and empirically demonstrate improved goal-reaching performance and robustness over current RL algorithms in several benchmark tasks. ", "one-sentence_summary": "We present GCSL, a simple RL method that uses supervised learning to learn goal-reaching policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ghosh|learning_to_reach_goals_via_iterated_supervised_learning", "supplementary_material": "", "pdf": "/pdf/34b6d953408a7aaff3549569738b80162e8e6dbc.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nghosh2021learning,\ntitle={Learning to Reach Goals via Iterated Supervised Learning},\nauthor={Dibya Ghosh and Abhishek Gupta and Ashwin Reddy and Justin Fu and Coline Manon Devin and Benjamin Eysenbach and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rALA0Xo6yNJ}\n}"}}
{"forum": "m5Qsh0kBQG", "review_ratings": ["7: Good paper, accept", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "8: Top 50% of accepted papers, clear accept"], "decision": "Oral", "content": {"title": "Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients", "authorids": ["~Brenden_K_Petersen1", "landajuelala1@llnl.gov", "~Terrell_N._Mundhenk1", "santiago10@llnl.gov", "~Soo_Kyung_Kim1", "kim102@llnl.gov"], "authors": ["Brenden K Petersen", "Mikel Landajuela Larma", "Terrell N. Mundhenk", "Claudio Prata Santiago", "Soo Kyung Kim", "Joanne Taery Kim"], "keywords": ["symbolic regression", "reinforcement learning", "automated machine learning"], "abstract": "Discovering the underlying mathematical expressions describing a dataset is a core challenge for artificial intelligence. This is the problem of $\\textit{symbolic regression}$. Despite recent advances in training neural networks to solve complex tasks, deep learning approaches to symbolic regression are underexplored. We propose a framework that leverages deep learning for symbolic regression via a simple idea: use a large model to search the space of small models. Specifically, we use a recurrent neural network to emit a distribution over tractable mathematical expressions and employ a novel risk-seeking policy gradient to train the network to generate better-fitting expressions. Our algorithm outperforms several baseline methods (including Eureqa, the gold standard for symbolic regression) in its ability to exactly recover symbolic expressions on a series of benchmark problems, both with and without added noise. More broadly, our contributions include a framework that can be applied to optimize hierarchical, variable-length objects under a black-box performance metric, with the ability to incorporate constraints in situ, and a risk-seeking policy gradient formulation that optimizes for best-case performance instead of expected performance.", "one-sentence_summary": "A deep learning approach to symbolic regression, in which an autoregressive RNN emits a distribution over expressions that is optimized using a risk-seeking policy gradient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|deep_symbolic_regression_recovering_mathematical_expressions_from_data_via_riskseeking_policy_gradients", "supplementary_material": "", "pdf": "/pdf/317665469793748a5dbbedaa91f4f31e395d23bf.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npetersen2021deep,\ntitle={Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients},\nauthor={Brenden K Petersen and Mikel Landajuela Larma and Terrell N. Mundhenk and Claudio Prata Santiago and Soo Kyung Kim and Joanne Taery Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=m5Qsh0kBQG}\n}"}}
{"forum": "UuchYL8wSZo", "review_ratings": ["8: Top 50% of accepted papers, clear accept", "8: Top 50% of accepted papers, clear accept", "8: Top 50% of accepted papers, clear accept", "9: Top 15% of accepted papers, strong accept"], "decision": "Oral", "content": {"title": "Learning Generalizable Visual Representations via Interactive Gameplay", "authorids": ["~Luca_Weihs1", "~Aniruddha_Kembhavi1", "~Kiana_Ehsani1", "~Sarah_M_Pratt1", "winsonh@allenai.org", "alvaroh@allenai.org", "~Eric_Kolve1", "dustins@allenai.org", "~Roozbeh_Mottaghi1", "~Ali_Farhadi3"], "authors": ["Luca Weihs", "Aniruddha Kembhavi", "Kiana Ehsani", "Sarah M Pratt", "Winson Han", "Alvaro Herrasti", "Eric Kolve", "Dustin Schwenk", "Roozbeh Mottaghi", "Ali Farhadi"], "keywords": ["representation learning", "deep reinforcement learning", "computer vision"], "abstract": "A growing body of research suggests that embodied gameplay, prevalent not just in human cultures but across a variety of animal species including turtles and ravens, is critical in developing the neural flexibility for creative problem solving, decision making, and socialization. Comparatively little is known regarding the impact of embodied gameplay upon artificial agents. While recent work has produced agents proficient in abstract games, these environments are far removed the real world and thus these agents can provide little insight into the advantages of embodied play. Hiding games, such as hide-and-seek, played universally, provide a rich ground for studying the impact of embodied gameplay on representation learning in the context of perspective taking, secret keeping, and false belief understanding. Here we are the first to show that embodied adversarial reinforcement learning agents playing Cache, a variant of hide-and-seek, in a high fidelity, interactive, environment, learn generalizable representations of their observations encoding information such as object permanence, free space, and containment. Moving closer to biologically motivated learning strategies, our agents' representations, enhanced by intentionality and memory, are developed through interaction and play. These results serve as a model for studying how facets of vision develop through interaction, provide an experimental framework for assessing what is learned by artificial agents, and demonstrates the value of moving from large, static, datasets towards experiential, interactive, representation learning.", "one-sentence_summary": "We show the representation learned through interaction and gameplay generalizes better compared to passive and static representation learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "weihs|learning_generalizable_visual_representations_via_interactive_gameplay", "pdf": "/pdf/9136e8ff7bc8b8e82f73830b816f60fe258ec628.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nweihs2021learning,\ntitle={Learning Generalizable Visual Representations via Interactive Gameplay},\nauthor={Luca Weihs and Aniruddha Kembhavi and Kiana Ehsani and Sarah M Pratt and Winson Han and Alvaro Herrasti and Eric Kolve and Dustin Schwenk and Roozbeh Mottaghi and Ali Farhadi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=UuchYL8wSZo}\n}"}}
{"forum": "0-uUGPbIjD", "review_ratings": ["8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept"], "decision": "Oral", "content": {"title": "Human-Level Performance in No-Press Diplomacy via Equilibrium Search", "authorids": ["~Jonathan_Gray2", "~Adam_Lerer1", "~Anton_Bakhtin1", "~Noam_Brown2"], "authors": ["Jonathan Gray", "Adam Lerer", "Anton Bakhtin", "Noam Brown"], "keywords": ["multi-agent systems", "regret minimization", "no-regret learning", "game theory", "reinforcement learning"], "abstract": "Prior AI breakthroughs in complex games have focused on either the purely adversarial or purely cooperative settings. In contrast, Diplomacy is a game of shifting alliances that involves both cooperation and competition. For this reason, Diplomacy has proven to be a formidable research challenge. In this paper we describe an agent for the no-press variant of Diplomacy that combines supervised learning on human data with one-step lookahead search via regret minimization. Regret minimization techniques have been behind previous AI successes in adversarial games, most notably poker, but have not previously been shown to be successful in large-scale games involving cooperation. We show that our agent greatly exceeds the performance of past no-press Diplomacy bots, is unexploitable by expert humans, and ranks in the top 2% of human players when playing anonymous games on a popular Diplomacy website.", "one-sentence_summary": "We present an agent that approximates a one-step equilibrium in no-press Diplomacy using no-regret learning and show that it exceeds human-level performance", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gray|humanlevel_performance_in_nopress_diplomacy_via_equilibrium_search", "supplementary_material": "/attachment/3274b9f417e258f3486082e095b50b3399a6d629.zip", "pdf": "/pdf/59e1f6ceb25265194013bc67945a7001ef36ca84.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngray2021humanlevel,\ntitle={Human-Level Performance in No-Press Diplomacy via Equilibrium Search},\nauthor={Jonathan Gray and Adam Lerer and Anton Bakhtin and Noam Brown},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0-uUGPbIjD}\n}"}}
{"forum": "Ysuv-WOFeKR", "review_ratings": ["8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "9: Top 15% of accepted papers, strong accept"], "decision": "Oral", "content": {"title": "Parrot: Data-Driven Behavioral Priors for Reinforcement Learning", "authorids": ["~Avi_Singh1", "~Huihan_Liu1", "~Gaoyue_Zhou1", "~Albert_Yu1", "~Nicholas_Rhinehart1", "~Sergey_Levine1"], "authors": ["Avi Singh", "Huihan Liu", "Gaoyue Zhou", "Albert Yu", "Nicholas Rhinehart", "Sergey Levine"], "keywords": ["reinforcement learning", "imitation learning"], "abstract": "Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to learn. In other machine learning fields, such as natural language processing or computer vision, pre-training on large, previously collected datasets to bootstrap learning for new tasks has emerged as a powerful paradigm to reduce data requirements when learning a new task. In this paper, we ask the following question: how can we enable similarly useful pre-training for RL agents? We propose a method for pre-training behavioral priors that can capture complex input-output relationships observed in successful trials from a wide range of previously seen tasks, and we show how this learned prior can be used for rapidly learning new tasks without impeding the RL agent's ability to try out novel behaviors. We demonstrate the effectiveness of our approach in challenging robotic manipulation domains involving image observations and sparse reward functions, where our method outperforms prior works by a substantial margin. Additional materials can be found on our project website: https://sites.google.com/view/parrot-rl", "one-sentence_summary": "We propose a method for pre-training a prior for reinforcement learning using data from a diverse range of tasks, and use this prior to speed up learning of new tasks. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "singh|parrot_datadriven_behavioral_priors_for_reinforcement_learning", "pdf": "/pdf/7ace651a0c4f40194198a9ab3ea9aefadb191c77.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsingh2021parrot,\ntitle={Parrot: Data-Driven Behavioral Priors for Reinforcement Learning},\nauthor={Avi Singh and Huihan Liu and Gaoyue Zhou and Albert Yu and Nicholas Rhinehart and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ysuv-WOFeKR}\n}"}}
{"forum": "0XXpJ4OtjW", "review_ratings": ["9: Top 15% of accepted papers, strong accept", "6: Marginally above acceptance threshold", "7: Good paper, accept"], "decision": "Oral", "content": {"title": "Evolving Reinforcement Learning Algorithms", "authorids": ["~John_D_Co-Reyes1", "~Yingjie_Miao1", "~Daiyi_Peng1", "ereal@google.com", "~Quoc_V_Le1", "~Sergey_Levine1", "~Honglak_Lee2", "~Aleksandra_Faust1"], "authors": ["John D Co-Reyes", "Yingjie Miao", "Daiyi Peng", "Esteban Real", "Quoc V Le", "Sergey Levine", "Honglak Lee", "Aleksandra Faust"], "keywords": ["reinforcement learning", "evolutionary algorithms", "meta-learning", "genetic programming"], "abstract": "We propose a method for meta-learning reinforcement learning algorithms by searching over the space of computational graphs which compute the loss function for a value-based model-free RL agent to optimize. The learned algorithms are domain-agnostic and can generalize to new environments not seen during training. Our method can both learn from scratch and bootstrap off known existing algorithms, like DQN, enabling interpretable modifications which improve performance. Learning from scratch on simple classical control and gridworld tasks, our method rediscovers the temporal-difference (TD) algorithm. Bootstrapped from DQN, we highlight two learned algorithms which obtain good generalization performance over other classical control tasks, gridworld type tasks, and Atari games. The analysis of the learned algorithm behavior shows resemblance to recently proposed RL algorithms that address overestimation in value-based methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "coreyes|evolving_reinforcement_learning_algorithms", "one-sentence_summary": "We meta-learn RL algorithms by evolving computational graphs which compute the loss function for a value-based model-free RL agent to optimize.", "pdf": "/pdf/78e8fae1b2cfbbae3e7010ca2f27649cb057ae84.pdf", "supplementary_material": "/attachment/ee0e0d71258074de1386100e5e43ff26745650de.zip", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nco-reyes2021evolving,\ntitle={Evolving Reinforcement Learning Algorithms},\nauthor={John D Co-Reyes and Yingjie Miao and Daiyi Peng and Esteban Real and Quoc V Le and Sergey Levine and Honglak Lee and Aleksandra Faust},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0XXpJ4OtjW}\n}"}}
