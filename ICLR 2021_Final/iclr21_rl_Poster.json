{"forum": "CBmJwzneppz", "review_ratings": ["5: Marginally below acceptance threshold", "6: Marginally above acceptance threshold", "7: Good paper, accept", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Optimism in Reinforcement Learning with Generalized Linear Function Approximation", "authorids": ["~Yining_Wang1", "~Ruosong_Wang1", "~Simon_Shaolei_Du1", "~Akshay_Krishnamurthy1"], "authors": ["Yining Wang", "Ruosong Wang", "Simon Shaolei Du", "Akshay Krishnamurthy"], "keywords": ["reinforcement learning", "optimism", "exploration", "function approximation", "theory", "regret analysis", "provable sample efficiency"], "abstract": "We design a new provably efficient algorithm for episodic reinforcement learning with generalized linear function approximation. We analyze the algorithm under a new expressivity assumption that we call ``optimistic closure,'' which is strictly weaker than assumptions from prior analyses for the linear setting. With optimistic closure, we prove that our algorithm enjoys a regret bound of $\\widetilde{O}\\left(H\\sqrt{d^3 T}\\right)$ where $H$ is the horizon, $d$ is the dimensionality of the state-action features and $T$ is the number of episodes. This is the first statistically and computationally efficient algorithm for reinforcement learning with generalized linear functions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|optimism_in_reinforcement_learning_with_generalized_linear_function_approximation", "one-sentence_summary": "A provably efficient (statistically and computationally) algorithm for reinforcement learning with generalized linear function approximation and no explicit dynamics assumptions.", "pdf": "/pdf/a233880f2576e4e435aefcdb4dce873e39557072.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021optimism,\ntitle={Optimism in Reinforcement Learning with Generalized Linear Function Approximation},\nauthor={Yining Wang and Ruosong Wang and Simon Shaolei Du and Akshay Krishnamurthy},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=CBmJwzneppz}\n}"}}
{"forum": "SK7A5pdrgov", "review_ratings": ["6: Marginally above acceptance threshold", "4: Ok but not good enough - rejection", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept"], "decision": "Poster", "content": {"title": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning", "authorids": ["~Ossama_Ahmed1", "~Frederik_Tr\u00e4uble1", "~Anirudh_Goyal1", "~Alexander_Neitz1", "~Manuel_Wuthrich1", "~Yoshua_Bengio1", "~Bernhard_Sch\u00f6lkopf1", "~Stefan_Bauer1"], "authors": ["Ossama Ahmed", "Frederik Tr\u00e4uble", "Anirudh Goyal", "Alexander Neitz", "Manuel Wuthrich", "Yoshua Bengio", "Bernhard Sch\u00f6lkopf", "Stefan Bauer"], "keywords": ["reinforcement learning", "transfer learning", "sim2real transfer", "domain adaptation", "causality", "generalization", "robotics"], "abstract": "Despite recent successes of reinforcement learning (RL), it remains a challenge for agents to transfer learned skills to related environments. To facilitate research addressing this problem, we proposeCausalWorld, a benchmark for causal structure and transfer learning in a robotic manipulation environment. The environment is a simulation of an open-source robotic platform, hence offering the possibility of sim-to-real transfer. Tasks consist of constructing 3D shapes from a set of blocks - inspired by how children learn to build complex structures. The key strength of CausalWorld is that it provides a combinatorial family of such tasks with common causal structure and underlying factors (including, e.g., robot and object masses, colors, sizes). The user (or the agent) may intervene on all causal variables, which allows  for  fine-grained  control  over  how  similar  different  tasks  (or  task  distributions) are. One can thus easily define training and evaluation distributions of a desired difficulty level,  targeting a specific form of generalization (e.g.,  only changes in appearance or object mass). Further, this common parametrization facilitates defining curricula by interpolating between an initial and a target task. While users may define their own task distributions, we present eight meaningful distributions as concrete benchmarks, ranging from simple to very challenging, all of which require long-horizon planning as well as precise low-level motor control. Finally, we provide baseline results for a subset of these tasks on distinct training curricula and corresponding evaluation protocols, verifying the feasibility of the tasks in this benchmark.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ahmed|causalworld_a_robotic_manipulation_benchmark_for_causal_structure_and_transfer_learning", "one-sentence_summary": "A benchmark to address the challenge of agents transferring their learned skills to related environments; primarily for causal structure and transfer learning.", "pdf": "/pdf/824ca65f541287b48a971348ef2dff33ffce0ffd.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nahmed2021causalworld,\ntitle={CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning},\nauthor={Ossama Ahmed and Frederik Tr{\\\"a}uble and Anirudh Goyal and Alexander Neitz and Manuel Wuthrich and Yoshua Bengio and Bernhard Sch{\\\"o}lkopf and Stefan Bauer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=SK7A5pdrgov}\n}"}}
{"forum": "tc5qisoB-C", "review_ratings": ["6: Marginally above acceptance threshold", "7: Good paper, accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "4: Ok but not good enough - rejection"], "decision": "Poster", "content": {"title": "C-Learning: Learning to Achieve Goals via Recursive Classification", "authorids": ["~Benjamin_Eysenbach1", "~Ruslan_Salakhutdinov1", "~Sergey_Levine1"], "authors": ["Benjamin Eysenbach", "Ruslan Salakhutdinov", "Sergey Levine"], "keywords": ["reinforcement learning", "goal reaching", "density estimation", "Q-learning", "hindsight relabeling"], "abstract": "We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned reinforcement learning (RL), is centered around learning a conditional probability density function over future states. Instead of directly estimating this density function, we indirectly estimate this density function by training a classifier to predict whether an observation comes from the future. Via Bayes' rule, predictions from our classifier can be transformed into predictions over future states. Importantly, an off-policy variant of our algorithm allows us to predict the future state distribution of a new policy, without collecting new experience. This variant allows us to optimize functionals of a policy's future state distribution, such as the density of reaching a particular goal state. While conceptually similar to Q-learning, our work lays a principled foundation for goal-conditioned RL as density estimation, providing justification for goal-conditioned methods used in prior work. This foundation makes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we confirm experimentally. Moreover, our proposed method is competitive with prior goal-conditioned RL methods.", "one-sentence_summary": "We reframe the goal-conditioned RL problem as one of predicting and controlling the future state of the world, and derive a principled algorithm to solve this problem. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "eysenbach|clearning_learning_to_achieve_goals_via_recursive_classification", "pdf": "/pdf/6a06ad37cef81666dc0ffbc9cffba623fcb34843.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\neysenbach2021clearning,\ntitle={C-Learning: Learning to Achieve Goals via Recursive Classification},\nauthor={Benjamin Eysenbach and Ruslan Salakhutdinov and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tc5qisoB-C}\n}"}}
{"forum": "sCZbhBvqQaU", "review_ratings": ["6: Marginally above acceptance threshold", "7: Good paper, accept", "7: Good paper, accept", "7: Good paper, accept"], "decision": "Poster", "content": {"title": "Robust Reinforcement Learning on State Observations with Learned Optimal Adversary", "authorids": ["~Huan_Zhang1", "~Hongge_Chen1", "~Duane_S_Boning1", "~Cho-Jui_Hsieh1"], "authors": ["Huan Zhang", "Hongge Chen", "Duane S Boning", "Cho-Jui Hsieh"], "keywords": ["reinforcement learning", "robustness", "adversarial attacks", "adversarial defense"], "abstract": "We study the robustness of reinforcement learning (RL) with adversarially perturbed state observations, which aligns with the setting of many adversarial attacks to deep reinforcement learning (DRL) and is also important for rolling out real-world RL agent under unpredictable sensing noise. With a fixed agent policy, we demonstrate that an optimal adversary to perturb state observations can be found, which is guaranteed to obtain the worst case agent reward. For DRL settings, this leads to a novel empirical adversarial attack to RL agents via a learned adversary that is much stronger than previous ones. To enhance the robustness of an agent, we propose a framework of alternating training with learned adversaries (ATLA), which trains an adversary online together with the agent using policy gradient following the optimal adversarial attack framework. Additionally, inspired by the analysis of state-adversarial Markov decision process (SA-MDP), we show that past states and actions (history) can be useful for learning a robust agent, and we empirically find a LSTM based policy can be more robust under adversaries. Empirical evaluations on a few continuous control environments show that ATLA achieves state-of-the-art performance under strong adversaries. Our code is available at https://github.com/huanzhang12/ATLA_robust_RL.", "one-sentence_summary": "We study the robustness of RL agents under perturbations on states and find that using an \"optimal\" adversary learned online in an alternating training manner can improve the robustness of agent policy.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|robust_reinforcement_learning_on_state_observations_with_learned_optimal_adversary", "pdf": "/pdf/9a0def4f4b70bbb3d4c3157a3ee5e4110bb9363a.pdf", "supplementary_material": "", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nzhang2021robust,\ntitle={Robust Reinforcement Learning on State Observations with Learned Optimal Adversary},\nauthor={Huan Zhang and Hongge Chen and Duane S Boning and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=sCZbhBvqQaU}\n}"}}
{"forum": "TQt98Ya7UMP", "review_ratings": ["6: Marginally above acceptance threshold", "7: Good paper, accept", "7: Good paper, accept", "7: Good paper, accept"], "decision": "Poster", "content": {"title": "Balancing Constraints and Rewards with Meta-Gradient D4PG", "authorids": ["dancalian@google.com", "~Daniel_J_Mankowitz2", "~Tom_Zahavy2", "~Zhongwen_Xu1", "~Junhyuk_Oh2", "~Nir_Levine2", "~Timothy_Mann1"], "authors": ["Dan A. Calian", "Daniel J Mankowitz", "Tom Zahavy", "Zhongwen Xu", "Junhyuk Oh", "Nir Levine", "Timothy Mann"], "keywords": ["reinforcement learning", "meta-gradients", "constraints"], "abstract": "Deploying Reinforcement Learning (RL) agents to solve real-world applications often requires satisfying complex system constraints. Often the constraint thresholds are incorrectly set due to the complex nature of a system or the inability to verify the thresholds offline (e.g, no simulator or reasonable offline evaluation procedure exists). This results in solutions where a task cannot be solved without violating the constraints. However, in many real-world cases, constraint violations are undesirable yet they are not catastrophic, motivating the need for soft-constrained RL approaches. We present two soft-constrained RL approaches that utilize meta-gradients to find a good trade-off between expected return and minimizing constraint violations. We demonstrate the effectiveness of these approaches by showing that they consistently outperform the baselines across four different Mujoco domains.", "one-sentence_summary": "This paper uses meta-gradients to perform soft-constrained Reinforcement Learning (RL) optimization", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "calian|balancing_constraints_and_rewards_with_metagradient_d4pg", "supplementary_material": "/attachment/a73e717343fda631c0c18cb21f6a64ebf5bf80e1.zip", "pdf": "/pdf/c2bc1eac3b05c897508a2b6cf4f096a98dbcc8e2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncalian2021balancing,\ntitle={Balancing Constraints and Rewards with Meta-Gradient D4{\\{}PG{\\}}},\nauthor={Dan A. Calian and Daniel J Mankowitz and Tom Zahavy and Zhongwen Xu and Junhyuk Oh and Nir Levine and Timothy Mann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TQt98Ya7UMP}\n}"}}
{"forum": "E3Ys6a1NTGT", "review_ratings": ["6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "7: Good paper, accept"], "decision": "Poster", "content": {"title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization", "authorids": ["~Jacob_Buckman2", "cgel@openai.com", "~Marc_G_Bellemare1"], "authors": ["Jacob Buckman", "Carles Gelada", "Marc G Bellemare"], "keywords": ["deep learning", "reinforcement learning", "offline reinforcement learning"], "abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments.", "one-sentence_summary": "A unified conceptual and mathematical framework for fixed-dataset policy optimization algorithms, revealing the importance of uncertainty and pessimism.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buckman|the_importance_of_pessimism_in_fixeddataset_policy_optimization", "pdf": "/pdf/e3dfbddf4c9078c3d334b54122b25fd90aba6c9b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbuckman2021the,\ntitle={The Importance of Pessimism in Fixed-Dataset Policy Optimization},\nauthor={Jacob Buckman and Carles Gelada and Marc G Bellemare},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=E3Ys6a1NTGT}\n}"}}
{"forum": "Xv_s64FiXTv", "review_ratings": ["8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "8: Top 50% of accepted papers, clear accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold"], "decision": "Poster", "content": {"title": "Learning to Represent Action Values as a Hypergraph on the Action Vertices", "authorids": ["~Arash_Tavakoli1", "~Mehdi_Fatemi1", "~Petar_Kormushev1"], "authors": ["Arash Tavakoli", "Mehdi Fatemi", "Petar Kormushev"], "keywords": ["reinforcement learning", "structural credit assignment", "structural inductive bias", "multi-dimensional discrete action spaces", "learning action representations"], "abstract": "Action-value estimation is a critical component of many reinforcement learning (RL) methods whereby sample complexity relies heavily on how fast a good estimator for action value can be learned. By viewing this problem through the lens of representation learning, good representations of both state and action can facilitate action-value estimation. While advances in deep learning have seamlessly driven progress in learning state representations, given the specificity of the notion of agency to RL, little attention has been paid to learning action representations. We conjecture that leveraging the combinatorial structure of multi-dimensional action spaces is a key ingredient for learning good representations of action. To test this, we set forth the action hypergraph networks framework---a class of functions for learning action representations in multi-dimensional discrete action spaces with a structural inductive bias. Using this framework we realise an agent class based on a combination with deep Q-networks, which we dub hypergraph Q-networks. We show the effectiveness of our approach on a myriad of domains: illustrative prediction problems under minimal confounding effects, Atari 2600 games, and discretised physical control benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tavakoli|learning_to_represent_action_values_as_a_hypergraph_on_the_action_vertices", "one-sentence_summary": "This paper introduces a class of models, called action hypergraph networks, for action-value estimation by leveraging the combinatorial structure of multi-dimensional discrete action spaces.", "pdf": "/pdf/1ffba349384a4d39e139b3deafa3e80e587e2dc1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntavakoli2021learning,\ntitle={Learning to Represent Action Values as a Hypergraph on the Action Vertices},\nauthor={Arash Tavakoli and Mehdi Fatemi and Petar Kormushev},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Xv_s64FiXTv}\n}"}}
{"forum": "0oabwyZbOu", "review_ratings": ["9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "4: Ok but not good enough - rejection", "5: Marginally below acceptance threshold"], "decision": "Poster", "content": {"title": "Mastering Atari with Discrete World Models", "authorids": ["~Danijar_Hafner1", "~Timothy_P_Lillicrap1", "~Mohammad_Norouzi1", "~Jimmy_Ba1"], "authors": ["Danijar Hafner", "Timothy P Lillicrap", "Mohammad Norouzi", "Jimmy Ba"], "keywords": ["Atari", "world models", "model-based reinforcement learning", "reinforcement learning", "planning", "actor critic"], "abstract": "Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and exceeds the final performance of the top single-GPU agents IQN and Rainbow.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hafner|mastering_atari_with_discrete_world_models", "pdf": "/pdf/8d56b0de5fe3dd408bbfb0a196460637eef8a751.pdf", "one-sentence_summary": "DreamerV2 is the first agent based on a world model to achieve human-level performance on the Atari benchmark.", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhafner2021mastering,\ntitle={Mastering Atari with Discrete World Models},\nauthor={Danijar Hafner and Timothy P Lillicrap and Mohammad Norouzi and Jimmy Ba},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=0oabwyZbOu}\n}"}}
{"forum": "fmtSg8591Q", "review_ratings": ["7: Good paper, accept", "6: Marginally above acceptance threshold", "7: Good paper, accept", "7: Good paper, accept"], "decision": "Poster", "content": {"title": "Efficient Reinforcement Learning in Factored MDPs with Application to Constrained RL", "authorids": ["~Xiaoyu_Chen2", "~Jiachen_Hu1", "~Lihong_Li1", "~Liwei_Wang1"], "authors": ["Xiaoyu Chen", "Jiachen Hu", "Lihong Li", "Liwei Wang"], "keywords": ["reinforcement learning", "factored MDP", "constrained RL", "learning theory"], "abstract": "Reinforcement learning (RL) in episodic, factored Markov decision processes (FMDPs) is studied. We propose an algorithm called FMDP-BF, which leverages the factorization structure of FMDP.  The regret of FMDP-BF is shown to be exponentially smaller than that of optimal algorithms designed for non-factored MDPs, and improves on the best previous result for FMDPs~\\citep{osband2014near} by a factor of $\\sqrt{nH|\\mathcal{S}_i|}$, where $|\\mathcal{S}_i|$ is the cardinality of the factored state subspace, $H$ is the planning horizon and $n$ is the number of factored transition. To show the optimality of our bounds, we also provide a lower bound for FMDP, which indicates that our algorithm is near-optimal w.r.t. timestep $T$, horizon $H$ and factored state-action subspace cardinality. Finally, as an application, we study a new formulation of constrained RL, known as RL with knapsack constraints (RLwK), and provides the first sample-efficient algorithm based on FMDP-BF.", "one-sentence_summary": "We propose an efficient algorithm with near-optimal regret guarantee for factored MDP, and apply the algorithm to a new formulation of constrained RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chen|efficient_reinforcement_learning_in_factored_mdps_with_application_to_constrained_rl", "pdf": "/pdf/a8950d55072da9151823a07d4c8c83043c445db5.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchen2021efficient,\ntitle={Efficient Reinforcement Learning in Factored {\\{}MDP{\\}}s with Application to Constrained {\\{}RL{\\}}},\nauthor={Xiaoyu Chen and Jiachen Hu and Lihong Li and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fmtSg8591Q}\n}"}}
{"forum": "lvRTC669EY_", "review_ratings": ["6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "7: Good paper, accept", "5: Marginally below acceptance threshold"], "decision": "Poster", "content": {"title": "Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization", "authorids": ["tangzhenggang@pku.edu.cn", "yc19@mails.tsinghua.edu.cn", "~Boyuan_Chen2", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Fei_Fang1", "~Simon_Shaolei_Du1", "~Yu_Wang3", "~Yi_Wu1"], "authors": ["Zhenggang Tang", "Chao Yu", "Boyuan Chen", "Huazhe Xu", "Xiaolong Wang", "Fei Fang", "Simon Shaolei Du", "Yu Wang", "Yi Wu"], "keywords": ["strategic behavior", "multi-agent reinforcement learning", "reward randomization", "diverse strategies"], "abstract": "We propose a simple, general and effective technique, Reward Randomization for discovering diverse strategic policies in complex multi-agent games. Combining reward randomization and policy gradient, we derive a new algorithm, Reward-Randomized Policy Gradient (RPG). RPG is able to discover a set of multiple distinctive human-interpretable strategies in challenging temporal trust dilemmas, including grid-world games and a real-world game Agar.io, where multiple equilibria exist but standard multi-agent policy gradient algorithms always converge to a fixed one with a sub-optimal payoff for every player even using state-of-the-art exploration techniques. Furthermore, with the set of diverse strategies from RPG, we can (1) achieve higher payoffs by fine-tuning the best policy from the set; and (2) obtain an adaptive agent by using this set of strategies as its training opponents. ", "one-sentence_summary": "We propose an MARL algorithm, RPG, which discovers diverse non-trivial strategic behavior in several challenging multi-agent games.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tang|discovering_diverse_multiagent_strategic_behavior_via_reward_randomization", "pdf": "/pdf/2062fdf1e8a1dbc3c1d293239ad291f853463ba8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntang2021discovering,\ntitle={Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization},\nauthor={Zhenggang Tang and Chao Yu and Boyuan Chen and Huazhe Xu and Xiaolong Wang and Fei Fang and Simon Shaolei Du and Yu Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=lvRTC669EY_}\n}"}}
{"forum": "ETBc_MIMgoX", "review_ratings": ["7: Good paper, accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Learning with AMIGo: Adversarially Motivated Intrinsic Goals", "authorids": ["~Andres_Campero1", "~Roberta_Raileanu2", "~Heinrich_Kuttler1", "~Joshua_B._Tenenbaum1", "~Tim_Rockt\u00e4schel1", "~Edward_Grefenstette1"], "authors": ["Andres Campero", "Roberta Raileanu", "Heinrich Kuttler", "Joshua B. Tenenbaum", "Tim Rockt\u00e4schel", "Edward Grefenstette"], "keywords": ["reinforcement learning", "exploration", "meta-learning"], "abstract": "A key challenge for reinforcement learning (RL) consists of learning in environments with sparse extrinsic rewards. In contrast to current RL methods, humans are able to learn new skills with little or no reward by using various forms of intrinsic motivation. We propose AMIGo, a novel agent incorporating -- as form of meta-learning -- a goal-generating teacher that proposes Adversarially Motivated Intrinsic Goals to train a goal-conditioned \"student\" policy in the absence of (or alongside) environment reward. Specifically, through a simple but effective \"constructively adversarial\" objective, the teacher learns to propose increasingly challenging -- yet achievable -- goals that allow the student to learn general skills for acting in a new environment, independent of the task to be solved. We show that our method generates a natural curriculum of self-proposed goals which ultimately allows the agent to solve challenging procedurally-generated tasks where other forms of intrinsic motivation and state-of-the-art RL methods fail.", "one-sentence_summary": "A \"constructively adversarial\" teacher-student setup can augment on-policy algorithms to better solve difficult exploration tasks in RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "campero|learning_with_amigo_adversarially_motivated_intrinsic_goals", "pdf": "/pdf/2424de49f541b07d5245c6d926cf266b520cb248.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncampero2021learning,\ntitle={Learning with {\\{}AMIG{\\}}o: Adversarially Motivated Intrinsic Goals},\nauthor={Andres Campero and Roberta Raileanu and Heinrich Kuttler and Joshua B. Tenenbaum and Tim Rockt{\\\"a}schel and Edward Grefenstette},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ETBc_MIMgoX}\n}"}}
{"forum": "r-gPPHEjpmw", "review_ratings": ["4: Ok but not good enough - rejection", "4: Ok but not good enough - rejection", "7: Good paper, accept", "8: Top 50% of accepted papers, clear accept"], "decision": "Poster", "content": {"title": "Hierarchical Reinforcement Learning by Discovering Intrinsic Options", "authorids": ["~Jesse_Zhang3", "~Haonan_Yu5", "~Wei_Xu13"], "authors": ["Jesse Zhang", "Haonan Yu", "Wei Xu"], "keywords": ["hierarchical reinforcement learning", "reinforcement learning", "options", "unsupervised skill discovery", "exploration"], "abstract": "We propose a hierarchical reinforcement learning method, HIDIO, that can learn task-agnostic options in a self-supervised manner while jointly learning to utilize them to solve sparse-reward tasks. Unlike current hierarchical RL approaches that tend to formulate goal-reaching low-level tasks or pre-define ad hoc lower-level policies, HIDIO encourages lower-level option learning that is independent of the task at hand, requiring few assumptions or little knowledge about the task structure. These options are learned through an intrinsic entropy minimization objective conditioned on the option sub-trajectories. The learned options are diverse and task-agnostic. In experiments on sparse-reward robotic manipulation and navigation tasks, HIDIO achieves higher success rates with greater sample efficiency than regular RL baselines and two state-of-the-art hierarchical RL methods. Code at: https://github.com/jesbu1/hidio.", "one-sentence_summary": "Hierarchical RL that discovers short-horizon task-agnostic options to perform well on sparse reward manipulation and navigation tasks.", "pdf": "/pdf/8ab82acd2672b63eb1d694fcb5fc26a32c2f6d74.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|hierarchical_reinforcement_learning_by_discovering_intrinsic_options", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021hierarchical,\ntitle={Hierarchical Reinforcement Learning by Discovering Intrinsic Options},\nauthor={Jesse Zhang and Haonan Yu and Wei Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=r-gPPHEjpmw}\n}"}}
{"forum": "eqBwg3AcIAK", "review_ratings": ["8: Top 50% of accepted papers, clear accept", "6: Marginally above acceptance threshold", "7: Good paper, accept", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers", "authorids": ["~Benjamin_Eysenbach1", "~Shreyas_Chaudhari1", "~Swapnil_Asawa1", "~Sergey_Levine1", "~Ruslan_Salakhutdinov1"], "authors": ["Benjamin Eysenbach", "Shreyas Chaudhari", "Swapnil Asawa", "Sergey Levine", "Ruslan Salakhutdinov"], "keywords": ["reinforcement learning", "transfer learning", "domain adaptation"], "abstract": "We propose a simple, practical, and intuitive approach for domain adaptation in reinforcement learning. Our approach stems from the idea that the agent's experience in the source domain should look similar to its experience in the target domain. Building off of a probabilistic view of RL, we achieve this goal by compensating for the difference in dynamics by modifying the reward function. This modified reward function is simple to estimate by learning auxiliary classifiers that distinguish source-domain transitions from target-domain transitions. Intuitively, the agent is penalized for transitions that would indicate that the agent is interacting with the source domain, rather than the target domain. Formally, we prove that applying our method in the source domain is guaranteed to obtain a near-optimal policy for the target domain, provided that the source and target domains satisfy a lightweight assumption. Our approach is applicable to domains with continuous states and actions and does not require learning an explicit model of the dynamics. On discrete and continuous control tasks, we illustrate the mechanics of our approach and demonstrate its scalability to high-dimensional~tasks.", "one-sentence_summary": "We propose a method for addressing domain adaptation in RL by using a (learned) modified reward, and prove that our method recovers a near-optimal policy for the target domain.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "eysenbach|offdynamics_reinforcement_learning_training_for_transfer_with_domain_classifiers", "pdf": "/pdf/cc68f287d3416da646309bf4689396ac35e2be5c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\neysenbach2021offdynamics,\ntitle={Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers},\nauthor={Benjamin Eysenbach and Shreyas Chaudhari and Swapnil Asawa and Sergey Levine and Ruslan Salakhutdinov},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eqBwg3AcIAK}\n}"}}
{"forum": "P6_q1BRxY8Q", "review_ratings": ["6: Marginally above acceptance threshold", "7: Good paper, accept", "8: Top 50% of accepted papers, clear accept", "8: Top 50% of accepted papers, clear accept", "4: Ok but not good enough - rejection"], "decision": "Poster", "content": {"title": "Learning Safe Multi-agent Control with Decentralized Neural Barrier Certificates", "authorids": ["~Zengyi_Qin1", "~Kaiqing_Zhang3", "~Yuxiao_Chen1", "~Jingkai_Chen2", "~Chuchu_Fan2"], "authors": ["Zengyi Qin", "Kaiqing Zhang", "Yuxiao Chen", "Jingkai Chen", "Chuchu Fan"], "keywords": ["Multi-agent", "safe", "control barrier function", "reinforcement learning"], "abstract": "We study the multi-agent safe control problem where agents should avoid collisions to static obstacles and collisions with each other while reaching their goals. Our core idea is to learn the multi-agent control policy jointly with  learning the control barrier functions as safety certificates. We propose a new joint-learning framework that can be implemented in a decentralized fashion, which can adapt to an arbitrarily large number of agents. Building upon this framework, we further improve the scalability by  incorporating neural network architectures  that are invariant to the quantity and permutation of neighboring agents. In addition, we propose a new spontaneous policy refinement method to further enforce the certificate condition during testing. We provide extensive experiments to demonstrate that our method significantly outperforms other leading multi-agent control approaches in terms of maintaining safety and completing original tasks. Our approach also shows substantial generalization capability in that the control policy can be trained with 8 agents in one scenario, while being used on other scenarios with up to 1024 agents in complex multi-agent environments and dynamics. Videos and source code can be found at https://realm.mit.edu/blog/learning-safe-multi-agent-control-decentralized-neural-barrier-certificates.", "one-sentence_summary": "We propose a safe and remarkably scalable multi-agent control approach via jointly learning the policy and decentralized control barrier certificates.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qin|learning_safe_multiagent_control_with_decentralized_neural_barrier_certificates", "supplementary_material": "/attachment/b4608dcefced74800b8aa4789255d038eeb7c569.zip", "pdf": "/pdf/f4c73ba388f2e06c4eb91e11f62979b68fb177f2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nqin2021learning,\ntitle={Learning Safe Multi-agent Control with Decentralized Neural Barrier Certificates},\nauthor={Zengyi Qin and Kaiqing Zhang and Yuxiao Chen and Jingkai Chen and Chuchu Fan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=P6_q1BRxY8Q}\n}"}}
{"forum": "pzpytjk3Xb2", "review_ratings": ["6: Marginally above acceptance threshold", "7: Good paper, accept", "7: Good paper, accept", "7: Good paper, accept"], "decision": "Poster", "content": {"title": "Policy-Driven Attack: Learning to Query for Hard-label Black-box Adversarial Examples", "authorids": ["~Ziang_Yan1", "~Yiwen_Guo1", "~Jian_Liang3", "~Changshui_Zhang1"], "authors": ["Ziang Yan", "Yiwen Guo", "Jian Liang", "Changshui Zhang"], "keywords": ["hard-label attack", "black-box attack", "adversarial attack", "reinforcement learning"], "abstract": "To craft black-box adversarial examples, adversaries need to query the victim model and take proper advantage of its feedback. Existing black-box attacks generally suffer from high query complexity, especially when only the top-1 decision (i.e., the hard-label prediction) of the victim model is available.  In this paper, we propose a novel hard-label black-box attack named Policy-Driven Attack, to reduce the query complexity. Our core idea is to learn promising search directions of the adversarial examples using a well-designed policy network in a novel reinforcement learning formulation, in which the queries become more sensible. Experimental results demonstrate that our method can significantly reduce the query complexity in comparison with existing state-of-the-art hard-label black-box attacks on various image classification benchmark datasets. Code and models for reproducing our results are available at https://github.com/ZiangYan/pda.pytorch", "one-sentence_summary": "A novel hard-label black-box adversarial attack that introduces a reinforcement learning based formulation with a pre-trained policy network", "pdf": "/pdf/7095c4811250b0fb464739f87a3151931d53f718.pdf", "supplementary_material": "/attachment/3800fd04a440add61a2a45773f7afd6c2e519ca8.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yan|policydriven_attack_learning_to_query_for_hardlabel_blackbox_adversarial_examples", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyan2021policydriven,\ntitle={Policy-Driven Attack: Learning to Query for Hard-label Black-box Adversarial Examples},\nauthor={Ziang Yan and Yiwen Guo and Jian Liang and Changshui Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=pzpytjk3Xb2}\n}"}}
{"forum": "kmqjgSNXby", "review_ratings": ["7: Good paper, accept", "6: Marginally above acceptance threshold", "7: Good paper, accept"], "decision": "Poster", "content": {"title": "Autoregressive Dynamics Models for Offline Policy Evaluation and Optimization", "authorids": ["~Michael_R_Zhang1", "~Thomas_Paine1", "~Ofir_Nachum1", "~Cosmin_Paduraru1", "~George_Tucker1", "~ziyu_wang1", "~Mohammad_Norouzi1"], "authors": ["Michael R Zhang", "Thomas Paine", "Ofir Nachum", "Cosmin Paduraru", "George Tucker", "ziyu wang", "Mohammad Norouzi"], "keywords": ["Off-policy policy evaluation", "autoregressive models", "offline reinforcement learning", "policy optimization"], "abstract": "Standard dynamics models for continuous control make use of feedforward computation to predict the conditional distribution of next state and reward given current state and action using a multivariate Gaussian with a diagonal covariance structure. This modeling choice assumes that different dimensions of the next state and reward are conditionally independent given the current state and action and may be driven by the fact that fully observable physics-based simulation environments entail deterministic transition dynamics. In this paper, we challenge this conditional independence assumption and propose a family of expressive autoregressive dynamics models that generate different dimensions of the next state and reward sequentially conditioned on previous dimensions. We demonstrate that autoregressive dynamics models indeed outperform standard feedforward models in log-likelihood on heldout transitions. Furthermore, we compare different model-based and model-free off-policy evaluation (OPE) methods on RL Unplugged, a suite of offline MuJoCo datasets, and find that autoregressive dynamics models consistently outperform all baselines, achieving a new state-of-the-art. Finally, we show that autoregressive dynamics models are useful for offline policy optimization by serving as a way to enrich the replay buffer through data augmentation and improving performance using model-based planning.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|autoregressive_dynamics_models_for_offline_policy_evaluation_and_optimization", "one-sentence_summary": "We demonstrate autoregressive dynamics models outperform standard feedforward models and other baselines in offline policy evaluation and optimization.", "pdf": "/pdf/258fc8fbf3df2a9d9783d528b562f8f503fe1167.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021autoregressive,\ntitle={Autoregressive Dynamics Models for Offline Policy Evaluation and Optimization},\nauthor={Michael R Zhang and Thomas Paine and Ofir Nachum and Cosmin Paduraru and George Tucker and ziyu wang and Mohammad Norouzi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kmqjgSNXby}\n}"}}
{"forum": "fmOOI2a3tQP", "review_ratings": ["7: Good paper, accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "7: Good paper, accept"], "decision": "Poster", "content": {"title": "Learning Robust State Abstractions for Hidden-Parameter Block MDPs", "authorids": ["~Amy_Zhang1", "~Shagun_Sodhani1", "~Khimya_Khetarpal1", "~Joelle_Pineau1"], "authors": ["Amy Zhang", "Shagun Sodhani", "Khimya Khetarpal", "Joelle Pineau"], "keywords": ["multi-task reinforcement learning", "bisimulation", "hidden-parameter mdp", "block mdp"], "abstract": "Many control tasks exhibit similar dynamics that can be modeled as having common latent structure. Hidden-Parameter Markov Decision Processes (HiP-MDPs) explicitly model this structure to improve sample efficiency in multi-task settings.\nHowever, this setting makes strong assumptions on the observability of the state that limit its application in real-world scenarios with rich observation spaces.  In this work, we leverage ideas of common structure from the HiP-MDP setting, and extend it to enable robust state abstractions inspired by Block MDPs. We  derive instantiations of this new framework for  both multi-task reinforcement learning (MTRL) and  meta-reinforcement learning (Meta-RL) settings. Further, we provide transfer and generalization bounds based on task and state similarity, along with sample complexity bounds that depend on the aggregate number of samples across tasks, rather than the number of tasks, a significant improvement over prior work. To further demonstrate efficacy of the proposed method, we empirically compare and show improvement over multi-task and meta-reinforcement learning baselines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_robust_state_abstractions_for_hiddenparameter_block_mdps", "pdf": "/pdf/acd2e5fe7849505e5c63b1341d5225ee12a494be.pdf", "one-sentence_summary": "We propose a new framework for tackling environments with different dynamics in rich observation settings and learning abstractions with this framework for improved generalization performance.", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning Robust State Abstractions for Hidden-Parameter Block {\\{}MDP{\\}}s},\nauthor={Amy Zhang and Shagun Sodhani and Khimya Khetarpal and Joelle Pineau},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fmOOI2a3tQP}\n}"}}
{"forum": "RqCC_00Bg7V", "review_ratings": ["6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "7: Good paper, accept"], "decision": "Poster", "content": {"title": "Blending MPC & Value Function Approximation for Efficient Reinforcement Learning", "authorids": ["~Mohak_Bhardwaj1", "sanjiban.choudhury@gmail.com", "~Byron_Boots1"], "authors": ["Mohak Bhardwaj", "Sanjiban Choudhury", "Byron Boots"], "keywords": ["reinforcement learning", "model-predictive control"], "abstract": "Model-Predictive Control (MPC) is a powerful tool for controlling complex, real-world systems that uses a model to make predictions about future behavior. For each state encountered, MPC solves an online optimization problem to choose a control action that will minimize future cost. This is a surprisingly effective strategy, but real-time performance requirements warrant the use of simple models. If the model is not sufficiently accurate, then the resulting controller can be biased, limiting performance. We present a framework for improving on MPC with model-free reinforcement learning (RL). The key insight is to view MPC as constructing a series of local Q-function approximations. We show that by using a parameter $\\lambda$, similar to the trace decay parameter in TD($\\lambda$), we can systematically trade-off learned value estimates against the local Q-function approximations. We present a theoretical analysis that shows how error from inaccurate models in MPC and value function estimation in RL can be balanced. We further propose an algorithm that changes $\\lambda$ over time to reduce the dependence on MPC as our estimates of the value function improve, and test the efficacy our approach on challenging high-dimensional manipulation tasks with biased models in simulation. We demonstrate that our approach can obtain performance comparable with MPC with access to true dynamics even under severe model bias and is more sample efficient as compared to model-free RL.", "one-sentence_summary": "A framework for blending model-predictive control and model-free value function learning to systematically trade-off bias due to approximate dynamics models and value functions learned from real data", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bhardwaj|blending_mpc_value_function_approximation_for_efficient_reinforcement_learning", "pdf": "/pdf/50c99bb8be8ec7784b7ca8b4a8b59da987b66045.pdf", "supplementary_material": "/attachment/2fed47454838141c4777c88d9216be907cb7f28c.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nbhardwaj2021blending,\ntitle={Blending {\\{}MPC{\\}} {\\&} Value Function Approximation for Efficient Reinforcement Learning},\nauthor={Mohak Bhardwaj and Sanjiban Choudhury and Byron Boots},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RqCC_00Bg7V}\n}"}}
{"forum": "p5uylG94S68", "review_ratings": ["7: Good paper, accept", "6: Marginally above acceptance threshold", "7: Good paper, accept", "7: Good paper, accept", "5: Marginally below acceptance threshold"], "decision": "Poster", "content": {"title": "Model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose?", "authorids": ["~Bal\u00e1zs_K\u00e9gl2", "gabriel.j.hurtado@gmail.com", "~Albert_Thomas1"], "authors": ["Bal\u00e1zs K\u00e9gl", "Gabriel Hurtado", "Albert Thomas"], "keywords": ["model-based reinforcement learning", "generative models", "mixture density nets", "dynamic systems", "heteroscedasticity"], "abstract": "We contribute to micro-data model-based reinforcement learning (MBRL) by rigorously comparing popular generative models using a fixed (random shooting) control agent. We find that on an environment that requires multimodal posterior predictives, mixture density nets outperform all other models by a large margin. When multimodality is not required, our surprising finding is that we do not need probabilistic posterior predictives: deterministic models are on par, in fact they consistently (although non-significantly) outperform their probabilistic counterparts. We also found that heteroscedasticity at training time, perhaps acting as a regularizer, improves predictions at longer horizons. At the methodological side, we design metrics and an experimental protocol which can be used to evaluate the various models, predicting their asymptotic performance when using them on the control problem. Using this framework, we improve the state-of-the-art sample complexity of MBRL on Acrobot by two to four folds, using an aggressive training schedule which is outside of the hyperparameter interval usually considered.", "one-sentence_summary": "Crucial model properties for model-based reinforcement learning: multi-modal posterior predictives and heteroscedasticity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "k\u00e9gl|modelbased_microdata_reinforcement_learning_what_are_the_crucial_model_properties_and_which_model_to_choose", "pdf": "/pdf/04313ea0678f51bf6e97525219f5b92003b041b9.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nk{\\'e}gl2021modelbased,\ntitle={Model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose?},\nauthor={Bal{\\'a}zs K{\\'e}gl and Gabriel Hurtado and Albert Thomas},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=p5uylG94S68}\n}"}}
{"forum": "5lhWG3Hj2By", "review_ratings": ["6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Enforcing robust control guarantees within neural network policies", "authorids": ["~Priya_L._Donti1", "mroderick@cmu.edu", "mahyarfa@seas.upenn.edu", "~J_Zico_Kolter1"], "authors": ["Priya L. Donti", "Melrose Roderick", "Mahyar Fazlyab", "J Zico Kolter"], "keywords": ["robust control", "reinforcement learning", "differentiable optimization"], "abstract": "When designing controllers for safety-critical systems, practitioners often face a challenging tradeoff between robustness and performance. While robust control methods provide rigorous guarantees on system stability under certain worst-case disturbances, they often yield simple controllers that perform poorly in the average (non-worst) case. In contrast, nonlinear control methods trained using deep learning have achieved state-of-the-art performance on many control tasks, but often lack robustness guarantees. In this paper, we propose a technique that combines the strengths of these two approaches: constructing a generic nonlinear control policy class, parameterized by neural networks, that nonetheless enforces the same provable robustness criteria as robust control. Specifically, our approach entails integrating custom convex-optimization-based projection layers into a neural network-based policy. We demonstrate the power of this approach on several domains, improving in average-case performance over existing robust control methods and in worst-case stability over (non-robust) deep RL methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "donti|enforcing_robust_control_guarantees_within_neural_network_policies", "one-sentence_summary": "We develop a generic nonlinear control policy class, parameterized by neural networks, that nonetheless enforces the same provable robustness criteria as robust control.", "supplementary_material": "", "pdf": "/pdf/c31f3f12091e950c7b2a58373e1af5225f1b5377.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndonti2021enforcing,\ntitle={Enforcing robust control guarantees within neural network policies},\nauthor={Priya L. Donti and Melrose Roderick and Mahyar Fazlyab and J Zico Kolter},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5lhWG3Hj2By}\n}"}}
{"forum": "OHgnfSrn2jv", "review_ratings": ["6: Marginally above acceptance threshold", "8: Top 50% of accepted papers, clear accept", "5: Marginally below acceptance threshold"], "decision": "Poster", "content": {"title": "Efficient Wasserstein Natural Gradients for Reinforcement Learning", "authorids": ["~Ted_Moskovitz1", "~Michael_Arbel1", "~Ferenc_Huszar1", "~Arthur_Gretton1"], "authors": ["Ted Moskovitz", "Michael Arbel", "Ferenc Huszar", "Arthur Gretton"], "keywords": ["reinforcement learning", "optimization"], "abstract": "A novel optimization approach is proposed for application to policy gradient methods and evolution strategies for reinforcement learning (RL). The procedure uses a computationally efficient \\emph{Wasserstein natural gradient} (WNG) descent that takes advantage of the geometry induced by a Wasserstein penalty to speed optimization. This method follows the recent theme in RL of including divergence penalties in the objective to establish trust regions. Experiments on challenging tasks demonstrate improvements in both computational cost and performance over advanced baselines. \n", "one-sentence_summary": "We develop novel, efficient estimators for the Wasserstein natural gradient applied to reinforcement learning that improve the efficiency and performance of advanced baselines.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "moskovitz|efficient_wasserstein_natural_gradients_for_reinforcement_learning", "supplementary_material": "/attachment/d61488838b20822f933deae31ba59fc558d11393.zip", "pdf": "/pdf/1c8cd7b02df8016f8cf8a6a2e70844ad5ae87de9.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmoskovitz2021efficient,\ntitle={Efficient Wasserstein Natural Gradients for Reinforcement Learning},\nauthor={Ted Moskovitz and Michael Arbel and Ferenc Huszar and Arthur Gretton},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OHgnfSrn2jv}\n}"}}
{"forum": "kBVJ2NtiY-", "review_ratings": ["5: Marginally below acceptance threshold", "7: Good paper, accept", "7: Good paper, accept", "5: Marginally below acceptance threshold"], "decision": "Poster", "content": {"title": "Learning What To Do by Simulating the Past", "authorids": ["~David_Lindner1", "~Rohin_Shah1", "~Pieter_Abbeel2", "~Anca_Dragan1"], "authors": ["David Lindner", "Rohin Shah", "Pieter Abbeel", "Anca Dragan"], "keywords": ["imitation learning", "reward learning", "reinforcement learning"], "abstract": "Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.", "one-sentence_summary": "Imitating policies given just a single state sampled from a rollout from an expert.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lindner|learning_what_to_do_by_simulating_the_past", "pdf": "/pdf/42dbf5584d4a07037b331115be1963b8650f85fd.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlindner2021learning,\ntitle={Learning What To Do by Simulating the Past},\nauthor={David Lindner and Rohin Shah and Pieter Abbeel and Anca Dragan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kBVJ2NtiY-}\n}"}}
{"forum": "jP1vTH3inC", "review_ratings": ["6: Marginally above acceptance threshold", "7: Good paper, accept", "7: Good paper, accept", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Discovering Non-monotonic Autoregressive Orderings with Variational Inference", "authorids": ["~Xuanlin_Li1", "~Brandon_Trabucco1", "~Dong_Huk_Park2", "~Michael_Luo2", "~Sheng_Shen2", "~Trevor_Darrell2", "~Yang_Gao1"], "authors": ["Xuanlin Li", "Brandon Trabucco", "Dong Huk Park", "Michael Luo", "Sheng Shen", "Trevor Darrell", "Yang Gao"], "keywords": ["variational inference", "unsupervised learning", "computer vision", "natural language processing", "optimization", "reinforcement learning"], "abstract": "The predominant approach for language modeling is to encode a sequence of tokens from left to right, but this eliminates a source of information: the order by which the sequence was naturally generated. One strategy to recover this information is to decode both the content and ordering of tokens. Some prior work supervises content and ordering with hand-designed loss functions to encourage specific orders or bootstraps from a predefined ordering. These approaches require domain-specific insight. Other prior work searches over valid insertion operations that lead to ground truth sequences during training, which has high time complexity and cannot be efficiently parallelized. We address these limitations with an unsupervised learner that can be trained in a fully-parallelizable manner to discover high-quality autoregressive orders in a data driven way without a domain-specific prior. The learner is a neural network that performs variational inference with the autoregressive ordering as a latent variable. Since the corresponding variational lower bound is not differentiable, we develop a practical algorithm for end-to-end optimization using policy gradients. Strong empirical results with our solution on sequence modeling tasks suggest that our algorithm is capable of discovering various autoregressive orders for different sequences that are competitive with or even better than fixed orders.", "one-sentence_summary": "The paper proposes an unsupervised learner that discovers non-monotonic autoregressive orders for sequence generation through fully-parallelizable end-to-end training without domain-specific prior.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|discovering_nonmonotonic_autoregressive_orderings_with_variational_inference", "supplementary_material": "/attachment/8064a233b1b6c84a18bed1e56cce4684e3ba03be.zip", "pdf": "/pdf/a9bcdb59d5a61ce55316a3ef34787b838b592a4a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021discovering,\ntitle={Discovering Non-monotonic Autoregressive Orderings with Variational Inference},\nauthor={Xuanlin Li and Brandon Trabucco and Dong Huk Park and Michael Luo and Sheng Shen and Trevor Darrell and Yang Gao},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jP1vTH3inC}\n}"}}
{"forum": "qYZD-AO1Vn", "review_ratings": ["6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "7: Good paper, accept", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Differentiable Trust Region Layers for Deep Reinforcement Learning", "authorids": ["~Fabian_Otto1", "~Philipp_Becker1", "~Vien_Anh_Ngo1", "~Hanna_Carolin_Maria_Ziesche1", "~Gerhard_Neumann1"], "authors": ["Fabian Otto", "Philipp Becker", "Vien Anh Ngo", "Hanna Carolin Maria Ziesche", "Gerhard Neumann"], "keywords": ["reinforcement learning", "trust region", "policy gradient", "projection", "Wasserstein distance", "Kullback-Leibler divergence", "Frobenius norm"], "abstract": "Trust region methods are a popular tool in reinforcement learning as they yield robust policy updates in continuous and discrete action spaces. However, enforcing such trust regions in deep reinforcement learning is difficult. Hence, many approaches, such as Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), are based on approximations. Due to those approximations, they violate the constraints or fail to find the optimal solution within the trust region. Moreover, they are difficult to implement, often lack sufficient exploration, and have been shown to depend on seemingly unrelated implementation choices. In this work, we propose differentiable neural network layers to enforce trust regions for deep Gaussian policies via closed-form projections. Unlike existing methods, those layers formalize trust regions for each state individually and can complement existing reinforcement learning algorithms. We derive trust region projections based on the Kullback-Leibler divergence, the Wasserstein L2 distance, and the Frobenius norm for Gaussian distributions. We empirically demonstrate that those projection layers achieve similar or better results than existing methods while being almost agnostic to specific implementation choices. The code is available at https://git.io/Jthb0.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "otto|differentiable_trust_region_layers_for_deep_reinforcement_learning", "pdf": "/pdf/16d7f0bd8f02047e8ca8dbcf7a7f000f5d685020.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\notto2021differentiable,\ntitle={Differentiable Trust Region Layers for Deep Reinforcement Learning},\nauthor={Fabian Otto and Philipp Becker and Vien Anh Ngo and Hanna Carolin Maria Ziesche and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qYZD-AO1Vn}\n}"}}
{"forum": "w2Z2OwVNeK", "review_ratings": ["7: Good paper, accept", "6: Marginally above acceptance threshold", "7: Good paper, accept", "3: Clear rejection"], "decision": "Poster", "content": {"title": "Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks", "authorids": ["~Ingmar_Schubert1", "~Ozgur_S_Oguz1", "~Marc_Toussaint1"], "authors": ["Ingmar Schubert", "Ozgur S Oguz", "Marc Toussaint"], "keywords": ["reinforcement learning", "reward shaping", "plan-based reward shaping", "robotics", "robotic manipulation"], "abstract": "In high-dimensional state spaces, the usefulness of Reinforcement Learning (RL) is limited by the problem of exploration. This issue has been addressed using potential-based reward shaping (PB-RS) previously. In the present work, we introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the strict optimality guarantees of PB-RS to a guarantee of preserved long-term behavior. Being less restrictive, FV-RS allows for reward shaping functions that are even better suited for improving the sample efficiency of RL algorithms. In particular, we consider settings in which the agent has access to an approximate plan. Here, we use examples of simulated robotic manipulation tasks to demonstrate that plan-based FV-RS can indeed significantly improve the sample efficiency of RL over plan-based PB-RS.", "one-sentence_summary": "We introduce Final-Volume-Preserving Reward Shaping, and show in a plan-based setting that it significantly increases the sample efficiency of reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "schubert|planbased_relaxed_reward_shaping_for_goaldirected_tasks", "supplementary_material": "/attachment/694bc96438e42ceda80cd46690b0ae34f9eed2d8.zip", "pdf": "/pdf/6ab6b9e3a9fe5a364f986aaff177de866990899b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nschubert2021planbased,\ntitle={Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks},\nauthor={Ingmar Schubert and Ozgur S Oguz and Marc Toussaint},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=w2Z2OwVNeK}\n}"}}
{"forum": "jEYKjPE1xYN", "review_ratings": ["8: Top 50% of accepted papers, clear accept", "6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Symmetry-Aware Actor-Critic for 3D Molecular Design", "authorids": ["~Gregor_N._C._Simm1", "~Robert_Pinsler1", "gc121@cam.ac.uk", "~Jos\u00e9_Miguel_Hern\u00e1ndez-Lobato1"], "authors": ["Gregor N. C. Simm", "Robert Pinsler", "G\u00e1bor Cs\u00e1nyi", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"], "keywords": ["deep reinforcement learning", "molecular design", "covariant neural networks"], "abstract": "Automating molecular design using deep reinforcement learning (RL) has the potential to greatly accelerate the search for novel materials. Despite recent progress on leveraging graph representations to design molecules, such methods are fundamentally limited by the lack of three-dimensional (3D) information. In light of this, we propose a novel actor-critic architecture for 3D molecular design that can generate molecular structures unattainable with previous approaches. This is achieved by exploiting the symmetries of the design process through a rotationally covariant state-action representation based on a spherical harmonics series expansion. We demonstrate the benefits of our approach on several 3D molecular design tasks, where we find that building in such symmetries significantly improves generalization and the quality of generated molecules.", "one-sentence_summary": "Covariant actor-critic based on spherical harmonics that exploits symmetries to design molecules in 3D", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "simm|symmetryaware_actorcritic_for_3d_molecular_design", "supplementary_material": "", "pdf": "/pdf/910f34d86d427e8bfed4af189076b9d497578643.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsimm2021symmetryaware,\ntitle={Symmetry-Aware Actor-Critic for 3D Molecular Design},\nauthor={Gregor N. C. Simm and Robert Pinsler and G{\\'a}bor Cs{\\'a}nyi and Jos{\\'e} Miguel Hern{\\'a}ndez-Lobato},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jEYKjPE1xYN}\n}"}}
{"forum": "--gvHfE3Xf5", "review_ratings": ["6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "7: Good paper, accept", "7: Good paper, accept"], "decision": "Poster", "content": {"title": "Meta-Learning of Structured Task Distributions in Humans and Machines", "authorids": ["~Sreejan_Kumar1", "~Ishita_Dasgupta1", "~Jonathan_Cohen1", "~Nathaniel_Daw1", "~Thomas_Griffiths1"], "authors": ["Sreejan Kumar", "Ishita Dasgupta", "Jonathan Cohen", "Nathaniel Daw", "Thomas Griffiths"], "keywords": ["meta-learning", "human cognition", "reinforcement learning", "compositionality"], "abstract": "In recent years, meta-learning, in which a model is trained on a family of tasks (i.e. a task distribution), has emerged as an approach to training neural networks to perform tasks that were previously assumed to require structured representations, making strides toward closing the gap between humans and machines. However, we argue that evaluating meta-learning remains a challenge, and can miss whether meta-learning actually uses the structure embedded within the tasks. These meta-learners might therefore still be significantly different from humans learners. To demonstrate this difference, we first define a new meta-reinforcement learning task in which a structured task distribution is generated using a compositional grammar. We then introduce a novel approach to constructing a \"null task distribution\" with the same statistical complexity as this structured task distribution but without the explicit rule-based structure used to generate the structured task. We train a standard meta-learning agent, a recurrent network trained with model-free reinforcement learning, and compare it with human performance across the two task distributions. We find a double dissociation in which humans do better in the structured task distribution whereas agents do better in the null task distribution -- despite comparable statistical complexity. This work highlights that multiple strategies can achieve reasonable meta-test performance, and that careful construction of control task distributions is a valuable way to understand which strategies meta-learners acquire, and how they might differ from humans. ", "one-sentence_summary": "We developed a novel meta-learning task with a structured task distribution and statistically equivalent \"null\" task distribution to show humans are more adept at the former whereas current meta-learning agents are more adept at the latter. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kumar|metalearning_of_structured_task_distributions_in_humans_and_machines", "supplementary_material": "/attachment/56b18093aa812df8d3743cb71d84ffa2cf535b30.zip", "pdf": "/pdf/ead1cbc3c9174506e0aa95f2a8fe5deb881931c1.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkumar2021metalearning,\ntitle={Meta-Learning of Structured Task Distributions in Humans and Machines},\nauthor={Sreejan Kumar and Ishita Dasgupta and Jonathan Cohen and Nathaniel Daw and Thomas Griffiths},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=--gvHfE3Xf5}\n}"}}
{"forum": "kWSeGEeHvF8", "review_ratings": ["7: Good paper, accept", "6: Marginally above acceptance threshold", "7: Good paper, accept", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Benchmarks for Deep Off-Policy Evaluation", "authorids": ["~Justin_Fu1", "~Mohammad_Norouzi1", "~Ofir_Nachum1", "~George_Tucker1", "~ziyu_wang1", "~Alexander_Novikov1", "~Mengjiao_Yang1", "~Michael_R_Zhang1", "~Yutian_Chen1", "~Aviral_Kumar2", "~Cosmin_Paduraru1", "~Sergey_Levine1", "~Thomas_Paine1"], "authors": ["Justin Fu", "Mohammad Norouzi", "Ofir Nachum", "George Tucker", "ziyu wang", "Alexander Novikov", "Mengjiao Yang", "Michael R Zhang", "Yutian Chen", "Aviral Kumar", "Cosmin Paduraru", "Sergey Levine", "Thomas Paine"], "keywords": ["reinforcement learning", "off-policy evaluation", "benchmarks"], "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.    ", "one-sentence_summary": "A benchmark proposal for off-policy evaluation and policy selection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fu|benchmarks_for_deep_offpolicy_evaluation", "supplementary_material": "/attachment/57358bb4d5338b0b3664c51b8e1b2cf50cee44e8.zip", "pdf": "/pdf/3a90850ebecc25b81a9534180c75842a2b672812.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfu2021benchmarks,\ntitle={Benchmarks for Deep Off-Policy Evaluation},\nauthor={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kWSeGEeHvF8}\n}"}}
{"forum": "V6BjBgku7Ro", "review_ratings": ["6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Planning from Pixels using Inverse Dynamics Models", "authorids": ["~Keiran_Paster1", "~Sheila_A._McIlraith1", "~Jimmy_Ba1"], "authors": ["Keiran Paster", "Sheila A. McIlraith", "Jimmy Ba"], "keywords": ["model based reinforcement learning", "deep reinforcement learning", "multi-task learning", "deep learning", "goal-conditioned reinforcement learning"], "abstract": "Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paster|planning_from_pixels_using_inverse_dynamics_models", "one-sentence_summary": "GLAMOR learns a latent world model by learning to predict action sequences conditioned on task completion.", "pdf": "/pdf/e1667f4513f3892a3eac139e23ee5198363e6741.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaster2021planning,\ntitle={Planning from Pixels using Inverse Dynamics Models},\nauthor={Keiran Paster and Sheila A. McIlraith and Jimmy Ba},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=V6BjBgku7Ro}\n}"}}
{"forum": "8xLkv08d70T", "review_ratings": ["6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "4: Ok but not good enough - rejection", "7: Good paper, accept"], "decision": "Poster", "content": {"title": "Adaptive Procedural Task Generation for Hard-Exploration Problems", "authorids": ["~Kuan_Fang3", "~Yuke_Zhu1", "~Silvio_Savarese1", "~Fei-Fei_Li1"], "authors": ["Kuan Fang", "Yuke Zhu", "Silvio Savarese", "Fei-Fei Li"], "keywords": ["reinforcement learning", "curriculum learning", "procedural generation", "task generation"], "abstract": "We introduce Adaptive Procedural Task Generation (APT-Gen), an approach to progressively generate a sequence of tasks as curricula to facilitate reinforcement learning in hard-exploration problems. At the heart of our approach, a task generator learns to create tasks from a parameterized task space via a black-box procedural generation module. To enable curriculum learning in the absence of a direct indicator of learning progress, we propose to train the task generator by balancing the agent's performance in the generated tasks and the similarity to the target tasks. Through adversarial training, the task similarity is adaptively estimated by a task discriminator defined on the agent's experiences, allowing the generated tasks to approximate target tasks of unknown parameterization or outside of the predefined task space. Our experiments on the grid world and robotic manipulation task domains show that APT-Gen achieves substantially better performance than various existing baselines by generating suitable tasks of rich variations.", "one-sentence_summary": "We propose a framework which creates tasks as curricula via procedural generation to expedite reinforcement learning in hard-exploration problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fang|adaptive_procedural_task_generation_for_hardexploration_problems", "supplementary_material": "", "pdf": "/pdf/24bbbe680bd44c907aab36d5e18bae82a7a5a48f.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfang2021adaptive,\ntitle={Adaptive Procedural Task Generation for Hard-Exploration Problems},\nauthor={Kuan Fang and Yuke Zhu and Silvio Savarese and Fei-Fei Li},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=8xLkv08d70T}\n}"}}
{"forum": "TBIzh9b5eaz", "review_ratings": ["6: Marginally above acceptance threshold", "8: Top 50% of accepted papers, clear accept", "5: Marginally below acceptance threshold", "6: Marginally above acceptance threshold", "7: Good paper, accept"], "decision": "Poster", "content": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}}
{"forum": "7_G8JySGecm", "review_ratings": ["4: Ok but not good enough - rejection", "7: Good paper, accept", "7: Good paper, accept", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Monte-Carlo Planning and Learning with Language Action Value Estimates", "authorids": ["~Youngsoo_Jang2", "~Seokin_Seo1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Youngsoo Jang", "Seokin Seo", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["natural language processing", "Monte-Carlo tree search", "reinforcement learning", "interactive fiction"], "abstract": "Interactive Fiction (IF) games provide a useful testbed for language-based reinforcement learning agents, posing significant challenges of natural language understanding, commonsense reasoning, and non-myopic planning in the combinatorial search space. Agents based on standard planning algorithms struggle to play IF games due to the massive search space of language actions. Thus, language-grounded planning is a key ability of such agents, since inferring the consequence of language action based on semantic understanding can drastically improve search. In this paper, we introduce Monte-Carlo planning with Language Action Value Estimates (MC-LAVE) that combines a Monte-Carlo tree search with language-driven exploration. MC-LAVE invests more search effort into semantically promising language actions using locally optimistic language value estimates, yielding a significant reduction in the effective search space of language actions. We then present a reinforcement learning approach via MC-LAVE, which alternates between MC-LAVE planning and supervised learning of the self-generated language actions. In the experiments, we demonstrate that our method achieves new high scores in various IF games.", "one-sentence_summary": "We present Monte-Carlo planning with Language Action Value Estimates (MC-LAVE) that combines a Monte-Carlo tree search with language-driven exploration for Interactive Fiction games.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jang|montecarlo_planning_and_learning_with_language_action_value_estimates", "pdf": "/pdf/255385188b591f81f5ec4cb8c99ea2b92467f6be.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njang2021montecarlo,\ntitle={Monte-Carlo Planning and Learning with Language Action Value Estimates},\nauthor={Youngsoo Jang and Seokin Seo and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7_G8JySGecm}\n}"}}
{"forum": "gJYlaqL8i8", "review_ratings": ["7: Good paper, accept", "6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Learning to Sample with Local and Global Contexts  in Experience Replay Buffer", "authorids": ["~Youngmin_Oh2", "~Kimin_Lee1", "~Jinwoo_Shin1", "~Eunho_Yang1", "~Sung_Ju_Hwang1"], "authors": ["Youngmin Oh", "Kimin Lee", "Jinwoo Shin", "Eunho Yang", "Sung Ju Hwang"], "keywords": ["reinforcement learning", "experience replay buffer", "off-policy RL"], "abstract": "Experience replay, which enables the agents to remember and reuse experience from the past, has played a significant role in the success of off-policy reinforcement learning (RL). To utilize the experience replay efficiently, the existing sampling methods allow selecting out more meaningful experiences by imposing priorities on them based on certain metrics (e.g. TD-error). However, they may result in sampling highly biased, redundant transitions since they compute the sampling rate for each transition independently, without consideration of its importance in relation to other transitions. In this paper, we aim to address the issue by proposing a new learning-based sampling method that can compute the relative importance of transition. To this end, we design a novel permutation-equivariant neural architecture that takes contexts from not only features of each transition (local) but also those of others (global) as inputs. We validate our framework, which we refer to as Neural Experience Replay Sampler (NERS), on multiple benchmark tasks for both continuous and discrete control tasks and show that it can significantly improve the performance of various off-policy RL methods. Further analysis confirms that the improvements of the sample efficiency indeed are due to sampling diverse and meaningful transitions by NERS that considers both local and global contexts. ", "one-sentence_summary": "We propose a learning-based neural replay which calculates the relative importance to sample experience for off-policy RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "oh|learning_to_sample_with_local_and_global_contexts_in_experience_replay_buffer", "supplementary_material": "/attachment/18c6c99e896fad9826dc40c5605e2d5b346c4ee2.zip", "pdf": "/pdf/92ef8e632b99778a17bd8e0187962812d2cd42c5.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\noh2021learning,\ntitle={Learning to Sample with Local and Global Contexts  in Experience Replay Buffer},\nauthor={Youngmin Oh and Kimin Lee and Jinwoo Shin and Eunho Yang and Sung Ju Hwang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gJYlaqL8i8}\n}"}}
{"forum": "P0p33rgyoE", "review_ratings": ["6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Variational Intrinsic Control Revisited", "authorids": ["~Taehwan_Kwon1"], "authors": ["Taehwan Kwon"], "keywords": ["Unsupervised reinforcement learning", "Information theory"], "abstract": "In this paper, we revisit variational intrinsic control (VIC), an unsupervised reinforcement learning method for finding the largest set of intrinsic options available to an agent. In the original work by Gregor et al. (2016), two VIC algorithms were proposed: one that represents the options explicitly, and the other that does it implicitly. We show that the intrinsic reward used in the latter is subject to bias in stochastic environments, causing convergence to suboptimal solutions. To correct this behavior, we propose two methods respectively based on the transitional probability model and Gaussian Mixture Model. We substantiate our claims through rigorous mathematical derivations and experimental analyses. ", "one-sentence_summary": "Revisitation of Variational Intrinsic Control (VIC) for the optimal behavior of implicit VIC under stochastic dynamics.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kwon|variational_intrinsic_control_revisited", "pdf": "/pdf/8841dcedad713be63398c9001418c334c7479b4e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkwon2021variational,\ntitle={Variational Intrinsic Control Revisited},\nauthor={Taehwan Kwon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=P0p33rgyoE}\n}"}}
{"forum": "_TM6rT7tXke", "review_ratings": ["7: Good paper, accept", "6: Marginally above acceptance threshold", "7: Good paper, accept", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Return-Based Contrastive Representation Learning for Reinforcement  Learning", "authorids": ["~Guoqing_Liu3", "~Chuheng_Zhang1", "~Li_Zhao1", "~Tao_Qin1", "~Jinhua_Zhu1", "~Li_Jian1", "~Nenghai_Yu1", "~Tie-Yan_Liu1"], "authors": ["Guoqing Liu", "Chuheng Zhang", "Li Zhao", "Tao Qin", "Jinhua Zhu", "Li Jian", "Nenghai Yu", "Tie-Yan Liu"], "keywords": ["reinforcement learning", "auxiliary task", "representation learning", "contrastive learning"], "abstract": "Recently, various auxiliary tasks have been proposed to accelerate representation learning and improve sample efficiency in deep reinforcement learning (RL). However, existing auxiliary tasks do not take the characteristics of RL problems into consideration and are unsupervised. By leveraging returns, the most important feedback signals in RL, we propose a novel auxiliary task that forces the learnt representations to discriminate state-action pairs with different returns. Our auxiliary loss is theoretically justified to learn representations that capture the structure of a new form of state-action abstraction, under which state-action pairs with similar return distributions are aggregated together. Empirically, our algorithm outperforms strong baselines on complex tasks in Atari games and DeepMind Control suite, and achieves even better performance when combined with existing auxiliary tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|returnbased_contrastive_representation_learning_for_reinforcement_learning", "one-sentence_summary": "We propose a novel contrastive learning based auxiliary task which forces the learnt representations to discriminate state-action pairs with different returns and achieve superior performance on complex tasks in terms of sample effiency.", "supplementary_material": "/attachment/4b6f310948f1f6a076655695088b20f00c017708.zip", "pdf": "/pdf/da82358af2f47721465fefc3dffd1bd3f3f2c16e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliu2021returnbased,\ntitle={Return-Based Contrastive Representation Learning for Reinforcement  Learning},\nauthor={Guoqing Liu and Chuheng Zhang and Li Zhao and Tao Qin and Jinhua Zhu and Li Jian and Nenghai Yu and Tie-Yan Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_TM6rT7tXke}\n}"}}
{"forum": "4qR3coiNaIv", "review_ratings": ["7: Good paper, accept", "6: Marginally above acceptance threshold", "7: Good paper, accept", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Scalable Bayesian Inverse Reinforcement Learning", "authorids": ["~Alex_James_Chan1", "~Mihaela_van_der_Schaar2"], "authors": ["Alex James Chan", "Mihaela van der Schaar"], "keywords": ["Bayesian", "Inverse reinforcement learning", "Imitation Learning"], "abstract": "Bayesian inference over the reward presents an ideal solution to the ill-posed nature of the inverse reinforcement learning problem. Unfortunately current methods generally do not scale well beyond the small tabular setting due to the need for an inner-loop MDP solver, and even non-Bayesian methods that do themselves scale often require extensive interaction with the environment to perform well, being inappropriate for high stakes or costly applications such as healthcare. In this paper we introduce our method, Approximate Variational Reward Imitation Learning (AVRIL), that addresses both of these issues by jointly learning an approximate posterior distribution over the reward that scales to arbitrarily complicated state spaces alongside an appropriate policy in a completely offline manner through a variational approach to said latent reward. Applying our method to real medical data alongside classic control simulations, we demonstrate Bayesian reward inference in environments beyond the scope of current methods, as well as task performance competitive with focused offline imitation learning algorithms.", "one-sentence_summary": "A variational inference approach to Bayesian inverse reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chan|scalable_bayesian_inverse_reinforcement_learning", "pdf": "/pdf/a94f190029f9ebd5affafc141a770843d501a8a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchan2021scalable,\ntitle={Scalable Bayesian Inverse Reinforcement Learning},\nauthor={Alex James Chan and Mihaela van der Schaar},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=4qR3coiNaIv}\n}"}}
{"forum": "Qr0aRliE_Hb", "review_ratings": ["7: Good paper, accept", "6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Simple Augmentation Goes a Long Way: ADRL for DNN Quantization", "authorids": ["~Lin_Ning1", "~Guoyang_Chen1", "weifeng.z@alibaba-inc.com", "~Xipeng_Shen1"], "authors": ["Lin Ning", "Guoyang Chen", "Weifeng Zhang", "Xipeng Shen"], "keywords": ["Reinforcement Learning", "Quantization", "mixed precision", "augmented deep reinforcement learning", "DNN"], "abstract": "Mixed precision quantization improves DNN performance by assigning different layers with different bit-width values. Searching for the optimal bit-width for each layer, however, remains a challenge. Deep Reinforcement Learning (DRL) shows some recent promise. It however suffers instability due to function approximation errors, causing large variances in the early training stages, slow convergence, and suboptimal policies in the mixed-precision quantization problem. This paper proposes augmented DRL (ADRL) as a way to alleviate these issues. This new strategy augments the neural networks in DRL with a complementary scheme to boost the performance of learning. The paper examines the effectiveness of ADRL both analytically and empirically, showing that it can produce more accurate quantized models than the state of the art DRL-based quantization while improving the learning speed by 4.5-64 times. ", "one-sentence_summary": "Augments the neural networks in Deep Reinforcement Learning(DRL) with a complementary scheme to boost the performance of learning and solve the common low convergence problem in the early stage of DRL", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ning|simple_augmentation_goes_a_long_way_adrl_for_dnn_quantization", "pdf": "/pdf/4f1af14f420632aa60f163e48701a935fae3a547.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nning2021simple,\ntitle={Simple Augmentation Goes a Long Way: {\\{}ADRL{\\}} for {\\{}DNN{\\}} Quantization},\nauthor={Lin Ning and Guoyang Chen and Weifeng Zhang and Xipeng Shen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qr0aRliE_Hb}\n}"}}
{"forum": "W3Wf_wKmqm9", "review_ratings": ["6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "C-Learning: Horizon-Aware Cumulative Accessibility Estimation", "authorids": ["panteha@layer6.ai", "~Gabriel_Loaiza-Ganem1", "harry@layer6.ai", "~Anthony_L._Caterini1", "jesse@layer6.ai", "tong@layer6.ai", "~Animesh_Garg1"], "authors": ["Panteha Naderian", "Gabriel Loaiza-Ganem", "Harry J. Braviner", "Anthony L. Caterini", "Jesse C. Cresswell", "Tong Li", "Animesh Garg"], "keywords": ["reinforcement learning", "goal reaching", "Q-learning"], "abstract": "Multi-goal reaching is an important problem in reinforcement learning needed to achieve algorithmic generalization. Despite recent advances in this field, current algorithms suffer from three major challenges: high sample complexity, learning only a single way of reaching the goals,  and difficulties in solving complex motion planning tasks. In order to address these limitations, we introduce the concept of cumulative accessibility functions, which measure the reachability of a goal from a given state within a specified horizon. We show that these functions obey a recurrence relation, which enables learning from offline interactions. We also prove that optimal cumulative accessibility functions are monotonic in the planning horizon. Additionally, our method can trade off speed and reliability in goal-reaching by suggesting multiple paths to a single goal depending on the provided horizon. We evaluate our approach on a set of multi-goal discrete and continuous control tasks. We show that our method outperforms state-of-the-art goal-reaching algorithms in success rate, sample complexity, and path optimality. Our code is available at https://github.com/layer6ai-labs/CAE, and additional visualizations can be found at https://sites.google.com/view/learning-cae/.", "one-sentence_summary": "We introduce C-learning, a Q-learning inspired method to learn horizon-dependent policies for goal reaching.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "naderian|clearning_horizonaware_cumulative_accessibility_estimation", "pdf": "/pdf/a50d45f24c305299cc3ac8aff9cd1d83ec4861e5.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nnaderian2021clearning,\ntitle={C-Learning: Horizon-Aware Cumulative Accessibility Estimation},\nauthor={Panteha Naderian and Gabriel Loaiza-Ganem and Harry J. Braviner and Anthony L. Caterini and Jesse C. Cresswell and Tong Li and Animesh Garg},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=W3Wf_wKmqm9}\n}"}}
{"forum": "chPj_I5KMHG", "review_ratings": ["6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "7: Good paper, accept", "4: Ok but not good enough - rejection"], "decision": "Poster", "content": {"title": "Grounding Language to Autonomously-Acquired Skills via Goal Generation", "authorids": ["~Ahmed_Akakzia1", "~C\u00e9dric_Colas1", "~Pierre-Yves_Oudeyer1", "~Mohamed_CHETOUANI2", "~Olivier_Sigaud1"], "authors": ["Ahmed Akakzia", "C\u00e9dric Colas", "Pierre-Yves Oudeyer", "Mohamed CHETOUANI", "Olivier Sigaud"], "keywords": ["Deep reinforcement learning", "intrinsic motivations", "symbolic representations", "autonomous learning"], "abstract": "We are interested in the autonomous acquisition of repertoires of skills. Language-conditioned reinforcement learning (LC-RL) approaches are great tools in this quest, as they allow to express abstract goals as sets of constraints on the states. However, most LC-RL agents are not autonomous and cannot learn without external instructions and feedback. Besides, their direct language condition cannot account for the goal-directed behavior of pre-verbal infants and strongly limits the expression of behavioral diversity for a given language input. To resolve these issues, we propose a new conceptual approach to language-conditioned RL: the Language-Goal-Behavior architecture (LGB). LGB decouples skill learning and language grounding via an intermediate semantic representation of the world. To showcase the properties of LGB, we present a specific implementation called DECSTR. DECSTR is an intrinsically motivated learning agent endowed with an innate semantic representation describing spatial relations between physical objects. In a first stage G -> B, it freely explores its environment and targets self-generated semantic configurations. In a second stage (L -> G), it trains a language-conditioned  goal generator to generate semantic goals that match the constraints expressed in language-based inputs. We showcase the additional properties of LGB w.r.t. both an end-to-end LC-RL approach and a similar approach leveraging non-semantic, continuous intermediate representations. Intermediate semantic representations help satisfy language commands in a diversity of ways, enable strategy switching after a failure and facilitate language grounding.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "akakzia|grounding_language_to_autonomouslyacquired_skills_via_goal_generation", "one-sentence_summary": "We propose a new RL architecture called Language-Goal-Behavior that proposes to decouple skill learning and language grounding via the introduction of an intermediate semantic goal representation.", "supplementary_material": "/attachment/96f8bb2ef390c8d11cf48b70cd66b43e1dfc45b2.zip", "pdf": "/pdf/d347c6eea558834f6b578284633cc4f3f1a0c0a9.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nakakzia2021grounding,\ntitle={Grounding Language to Autonomously-Acquired Skills via Goal Generation},\nauthor={Ahmed Akakzia and C{\\'e}dric Colas and Pierre-Yves Oudeyer and Mohamed CHETOUANI and Olivier Sigaud},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=chPj_I5KMHG}\n}"}}
{"forum": "Nc3TJqbcl3", "review_ratings": ["6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold"], "decision": "Poster", "content": {"title": "Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers", "authorids": ["~Cristina_Pinneri1", "shambhuraj.sawant@tuebingen.mpg.de", "sebastian.blaes@tuebingen.mpg.de", "~Georg_Martius1"], "authors": ["Cristina Pinneri", "Shambhuraj Sawant", "Sebastian Blaes", "Georg Martius"], "keywords": ["reinforcement learning", "zero-order optimization", "policy learning", "model-based learning", "robotics", "model predictive control"], "abstract": "Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pinneri|extracting_strong_policies_for_robotics_tasks_from_zeroorder_trajectory_optimizers", "one-sentence_summary": "We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.", "pdf": "/pdf/1e36a9b55c2b184bab1395be47101e4beb882f41.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npinneri2021extracting,\ntitle={Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers},\nauthor={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Nc3TJqbcl3}\n}"}}
{"forum": "ONBPHFZ7zG4", "review_ratings": ["8: Top 50% of accepted papers, clear accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "5: Marginally below acceptance threshold", "8: Top 50% of accepted papers, clear accept"], "decision": "Poster", "content": {"title": "Temporally-Extended \u03b5-Greedy Exploration", "authorids": ["~Will_Dabney1", "~Georg_Ostrovski1", "~Andre_Barreto1"], "authors": ["Will Dabney", "Georg Ostrovski", "Andre Barreto"], "keywords": ["reinforcement learning", "exploration"], "abstract": "Recent work on exploration in reinforcement learning (RL) has led to a series of increasingly complex solutions to the problem. This increase in complexity often comes at the expense of generality. Recent empirical studies suggest that, when applied to a broader set of domains, some sophisticated exploration methods are outperformed by simpler counterparts, such as \u03b5-greedy. In this paper we propose an exploration algorithm that retains the simplicity of \u03b5-greedy while reducing dithering. We build on a simple hypothesis: the main limitation of \u03b5-greedy exploration is its lack of temporal persistence, which limits its ability to escape local optima. We propose a temporally extended form of \u03b5-greedy that simply repeats the sampled action for a random duration. It turns out that, for many duration distributions, this suffices to improve exploration on a large set of domains. Interestingly, a class of distributions inspired by ecological models of animal foraging behaviour yields particularly strong performance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dabney|temporallyextended_greedy_exploration", "one-sentence_summary": "We discuss a new framework for option-based exploration, present a thorough empirical study of a simple, generally applicable set of options within this framework, and observe improved performance over state-of-the-art agents and exploration methods.", "pdf": "/pdf/be288b1cdd527108548adea1d4d8319ce8a8eae8.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndabney2021temporallyextended,\ntitle={Temporally-Extended {\\ensuremath{\\varepsilon}}-Greedy Exploration},\nauthor={Will Dabney and Georg Ostrovski and Andre Barreto},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ONBPHFZ7zG4}\n}"}}
{"forum": "F-mvpFpn_0q", "review_ratings": ["4: Ok but not good enough - rejection", "7: Good paper, accept", "7: Good paper, accept", "8: Top 50% of accepted papers, clear accept"], "decision": "Poster", "content": {"title": "Rapid Task-Solving in Novel Environments", "authorids": ["~Samuel_Ritter1", "~Ryan_Faulkner2", "~Laurent_Sartran1", "~Adam_Santoro1", "~Matthew_Botvinick1", "~David_Raposo1"], "authors": ["Samuel Ritter", "Ryan Faulkner", "Laurent Sartran", "Adam Santoro", "Matthew Botvinick", "David Raposo"], "keywords": ["deep reinforcement learning", "meta learning", "deep learning", "exploration", "planning"], "abstract": "We propose the challenge of rapid task-solving in novel environments (RTS), wherein an agent must solve a series of tasks as rapidly as possible in an unfamiliar environment. An effective RTS agent must balance between exploring the unfamiliar environment and solving its current task, all while building a model of the new environment over which it can plan when faced with later tasks. While modern deep RL agents exhibit some of these abilities in isolation, none are suitable for the full RTS challenge. To enable progress toward RTS, we introduce two challenge domains: (1) a minimal RTS challenge called the Memory&Planning Game and (2) One-Shot StreetLearn Navigation, which introduces scale and complexity from real-world data. We demonstrate that state-of-the-art deep RL agents fail at RTS in both domains, and that this failure is due to an inability to plan over gathered knowledge. We develop Episodic Planning Networks (EPNs) and show that deep-RL agents with EPNs excel at RTS, outperforming the nearest baseline by factors of 2-3 and learning to navigate held-out StreetLearn maps within a single episode. We show that EPNs learn to execute a value iteration-like planning algorithm and that they generalize to situations beyond their training experience.", "one-sentence_summary": "Our agents meta-learn to explore, build models on-the-fly, and plan, enabling them to rapidly solve sequences of tasks in unfamiliar environments.", "pdf": "/pdf/2a2a34541a2b4e34e92e1050f5935a08cca0163b.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ritter|rapid_tasksolving_in_novel_environments", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nritter2021rapid,\ntitle={Rapid Task-Solving in Novel Environments},\nauthor={Samuel Ritter and Ryan Faulkner and Laurent Sartran and Adam Santoro and Matthew Botvinick and David Raposo},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=F-mvpFpn_0q}\n}"}}
{"forum": "LiX3ECzDPHZ", "review_ratings": ["8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "4: Ok but not good enough - rejection"], "decision": "Poster", "content": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "nhardy01@gmail.com", "nikhilesh.natraj@ucsf.edu", "karunesh.ganguly@ucsf.edu", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Nicholas Hardy", "Nikhilesh Natraj", "Karunesh Ganguly", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze or neural activity measured by a brain implant. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, a large-scale observational study on handwriting samples from 60 users, and a pilot study with one participant using an electrocorticography-based brain-computer interface. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs from a brain implant or webcam.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/b299e92f993e7b50461323c326ad974dc5b67e09.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Nicholas Hardy and Nikhilesh Natraj and Karunesh Ganguly and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}}
{"forum": "Rcmk0xxIQV", "review_ratings": ["6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "4: Ok but not good enough - rejection", "7: Good paper, accept"], "decision": "Poster", "content": {"title": "QPLEX: Duplex Dueling Multi-Agent Q-Learning", "authorids": ["~Jianhao_Wang1", "~Zhizhou_Ren1", "~Terry_Liu2", "~Yang_Yu5", "~Chongjie_Zhang1"], "authors": ["Jianhao Wang", "Zhizhou Ren", "Terry Liu", "Yang Yu", "Chongjie Zhang"], "keywords": ["Multi-agent reinforcement learning", "Value factorization", "Dueling structure"], "abstract": "We explore value-based multi-agent reinforcement learning (MARL) in the popular paradigm of centralized training with decentralized execution (CTDE). CTDE has an important concept, Individual-Global-Max (IGM) principle, which requires the consistency between joint and local action selections to support efficient local decision-making. However, in order to achieve scalability, existing MARL methods either limit representation expressiveness of their value function classes or relax the IGM consistency, which may suffer from instability risk or may not perform well in complex domains. This paper presents a novel MARL approach, called duPLEX dueling multi-agent Q-learning (QPLEX), which takes a duplex dueling network architecture to factorize the joint value function. This duplex dueling structure encodes the IGM principle into the neural network architecture and thus enables efficient value function learning. Theoretical analysis shows that QPLEX achieves a complete IGM function class. Empirical experiments on StarCraft II micromanagement tasks demonstrate that QPLEX significantly outperforms state-of-the-art baselines in both online and offline data collection settings, and also reveal that QPLEX achieves high sample efficiency and can benefit from offline datasets without additional online exploration.", "one-sentence_summary": "A novel multi-agent Q-learning algorithm with a complete IGM (Individual-Global-Max) function class.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|qplex_duplex_dueling_multiagent_qlearning", "supplementary_material": "/attachment/ea6e1b5160f28a9662999681e747ffac9c93c146.zip", "pdf": "/pdf/b346f46b6a8ed100f25b8d16fde7d2500965fd70.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021qplex,\ntitle={{\\{}QPLEX{\\}}: Duplex Dueling Multi-Agent Q-Learning},\nauthor={Jianhao Wang and Zhizhou Ren and Terry Liu and Yang Yu and Chongjie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Rcmk0xxIQV}\n}"}}
{"forum": "OMNB1G5xzd4", "review_ratings": ["5: Marginally below acceptance threshold", "5: Marginally below acceptance threshold", "7: Good paper, accept", "8: Top 50% of accepted papers, clear accept"], "decision": "Poster", "content": {"title": "Model-Based Offline Planning", "authorids": ["aarg@google.com", "~Gabriel_Dulac-Arnold1"], "authors": ["Arthur Argenson", "Gabriel Dulac-Arnold"], "keywords": ["off-line reinforcement learning", "model-based reinforcement learning", "model-based control", "reinforcement learning", "model predictive control", "robotics"], "abstract": "Offline learning is a key part of making reinforcement learning (RL) useable in real systems. Offline RL looks at scenarios where there is data from a system's operation, but no direct access to the system when learning a policy. Recent work on training RL policies from offline data has shown results both with model-free policies learned directly from the data, or with planning on top of learnt models of the data. Model-free policies tend to be more performant, but are more opaque, harder to command externally, and less easy to integrate into larger systems. We propose an offline learner that generates a model that can be used to control the system directly through planning. This allows us to have easily controllable policies directly from data, without ever interacting with the system. We show the performance of our algorithm, Model-Based Offline Planning (MBOP) on a series of robotics-inspired tasks, and demonstrate its ability leverage planning to respect environmental constraints. We are able to find near-optimal polices for certain simulated systems from as little as 50 seconds of real-time system interaction, and create zero-shot goal-conditioned policies on a series of environments.", "one-sentence_summary": "This approach adapts model-based reinforcement learning to offline regimes with little data, and shows state of the art control in offline scenarios.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "argenson|modelbased_offline_planning", "supplementary_material": "/attachment/452d20f08756e74fb292f0375b666cfd30aa6062.zip", "pdf": "/pdf/81c53a9c5e305d1d030b2fa5e47206fbd6535dcf.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nargenson2021modelbased,\ntitle={Model-Based Offline Planning},\nauthor={Arthur Argenson and Gabriel Dulac-Arnold},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OMNB1G5xzd4}\n}"}}
{"forum": "po-DLlBuAuz", "review_ratings": ["7: Good paper, accept", "9: Top 15% of accepted papers, strong accept", "6: Marginally above acceptance threshold", "4: Ok but not good enough - rejection"], "decision": "Poster", "content": {"title": "Batch Reinforcement Learning Through Continuation Method", "authorids": ["~Yijie_Guo1", "~Shengyu_Feng1", "~Nicolas_Le_Roux2", "~Ed_Chi1", "~Honglak_Lee2", "~Minmin_Chen1"], "authors": ["Yijie Guo", "Shengyu Feng", "Nicolas Le Roux", "Ed Chi", "Honglak Lee", "Minmin Chen"], "keywords": ["batch reinforcement learning", "continuation method", "relaxed regularization"], "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|batch_reinforcement_learning_through_continuation_method", "supplementary_material": "/attachment/cd6b4e8e5375df38f85f6cb9cb9ce88ad149d9e8.zip", "pdf": "/pdf/84a7a35d996f84ab9fbbbcabccbdc21f44f2ba68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021batch,\ntitle={Batch Reinforcement Learning Through Continuation Method},\nauthor={Yijie Guo and Shengyu Feng and Nicolas Le Roux and Ed Chi and Honglak Lee and Minmin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=po-DLlBuAuz}\n}"}}
{"forum": "cP5IcoAkfKa", "review_ratings": ["7: Good paper, accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection"], "decision": "Poster", "content": {"title": "Large Batch Simulation for Deep Reinforcement Learning", "authorids": ["~Brennan_Shacklett1", "~Erik_Wijmans1", "petrenko@usc.edu", "~Manolis_Savva1", "~Dhruv_Batra1", "~Vladlen_Koltun1", "~Kayvon_Fatahalian2"], "authors": ["Brennan Shacklett", "Erik Wijmans", "Aleksei Petrenko", "Manolis Savva", "Dhruv Batra", "Vladlen Koltun", "Kayvon Fatahalian"], "keywords": ["reinforcement learning", "simulation"], "abstract": "We accelerate deep reinforcement learning-based training in visually complex 3D environments by two orders of magnitude over prior work, realizing end-to-end training speeds of over 19,000 frames of experience per second on a single GPU and up to 72,000 frames per second on a single eight-GPU machine. The key idea of our approach is to design a 3D renderer and embodied navigation simulator around the principle of \u201cbatch simulation\u201d: accepting and executing large batches of requests simultaneously.  Beyond exposing large amounts of work at once, batch simulation allows implementations to amortize in-memory storage of scene assets, rendering work, data loading, and synchronization costs across many simulation requests, dramatically improving the number of simulated agents per GPU and overall simulation throughput.  To balance DNN inference and training costs with faster simulation, we also build a computationally efficient policy DNN that maintains high task performance, and modify training algorithms to maintain sample efficiency when training with large mini-batches. By combining batch simulation and DNN performance optimizations, we demonstrate that PointGoal navigation agents can be trained in complex 3D environments on a single GPU in 1.5 days to 97% of the accuracy of agents trained on a prior state-of-the-art system using a 64-GPU cluster over three days.  We provide open-source reference implementations of our batch 3D renderer and simulator to facilitate incorporation of these ideas into RL systems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shacklett|large_batch_simulation_for_deep_reinforcement_learning", "supplementary_material": "/attachment/732acec8ef82c4275c7e4f3836b266a5f82ed477.zip", "pdf": "/pdf/623f84dd47e44c85099947df02e289ec8005ddc3.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nshacklett2021large,\ntitle={Large Batch Simulation for Deep Reinforcement Learning},\nauthor={Brennan Shacklett and Erik Wijmans and Aleksei Petrenko and Manolis Savva and Dhruv Batra and Vladlen Koltun and Kayvon Fatahalian},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=cP5IcoAkfKa}\n}"}}
{"forum": "HIGSa_3kOx3", "review_ratings": ["6: Marginally above acceptance threshold", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold"], "decision": "Poster", "content": {"title": "Reset-Free Lifelong Learning with Skill-Space Planning", "authorids": ["~Kevin_Lu2", "~Aditya_Grover1", "~Pieter_Abbeel2", "~Igor_Mordatch4"], "authors": ["Kevin Lu", "Aditya Grover", "Pieter Abbeel", "Igor Mordatch"], "keywords": ["reset-free", "lifelong", "reinforcement learning"], "abstract": "The objective of \\textit{lifelong} reinforcement learning (RL) is to optimize agents which can continuously adapt and interact in changing environments. However, current RL approaches fail drastically when environments are non-stationary and interactions are non-episodic. We propose \\textit{Lifelong Skill Planning} (LiSP), an algorithmic framework for lifelong RL based on planning in an abstract space of higher-order skills. We learn the skills in an unsupervised manner using intrinsic rewards and plan over the learned skills using a learned dynamics model. Moreover, our framework permits skill discovery even from offline data, thereby reducing the need for excessive real-world interactions. We demonstrate empirically that LiSP successfully enables long-horizon planning and learns agents that can avoid catastrophic failures even in challenging non-stationary and non-episodic environments derived from gridworld and MuJoCo benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lu|resetfree_lifelong_learning_with_skillspace_planning", "supplementary_material": "/attachment/776cd2ce9090eae7ac082070cf8a2c536a396055.zip", "pdf": "/pdf/c2294e8113d0b33d3849f2a97396d946826c3de3.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlu2021resetfree,\ntitle={Reset-Free Lifelong Learning with Skill-Space Planning},\nauthor={Kevin Lu and Aditya Grover and Pieter Abbeel and Igor Mordatch},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=HIGSa_3kOx3}\n}"}}
{"forum": "8cpHIfgY4Dj", "review_ratings": ["5: Marginally below acceptance threshold", "7: Good paper, accept", "5: Marginally below acceptance threshold"], "decision": "Poster", "content": {"title": "FOCAL: Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization", "authorids": ["~Lanqing_Li1", "yangrui19@mails.tsinghua.edu.cn", "~Dijun_Luo1"], "authors": ["Lanqing Li", "Rui Yang", "Dijun Luo"], "keywords": ["offline/batch reinforcement learning", "meta-reinforcement learning", "multi-task reinforcement learning", "distance metric learning", "contrastive learning"], "abstract": "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|focal_efficient_fullyoffline_metareinforcement_learning_via_distance_metric_learning_and_behavior_regularization", "one-sentence_summary": "A novel model-free, end-to-end fully-offline meta-RL algorithm designed to maximize practicality, performance and sample/computational efficiency.", "supplementary_material": "", "pdf": "/pdf/05ca2214a3025e5b31919a2618a95a09d1c090dc.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021focal,\ntitle={{FOCAL}: Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization},\nauthor={Lanqing Li and Rui Yang and Dijun Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=8cpHIfgY4Dj}\n}"}}
{"forum": "qpsl2dR9twy", "review_ratings": ["6: Marginally above acceptance threshold", "4: Ok but not good enough - rejection", "5: Marginally below acceptance threshold", "6: Marginally above acceptance threshold"], "decision": "Poster", "content": {"title": "Communication in Multi-Agent Reinforcement Learning: Intention Sharing", "authorids": ["~Woojun_Kim1", "~Jongeui_Park1", "~Youngchul_Sung1"], "authors": ["Woojun Kim", "Jongeui Park", "Youngchul Sung"], "keywords": ["Multi-agent reinforcement learning", "communication", "intention", "attention"], "abstract": "Communication is one of the core components for learning coordinated behavior in multi-agent systems.\nIn this paper, we propose a new communication scheme named  Intention Sharing (IS) for multi-agent reinforcement learning in order to enhance the coordination among agents. In the proposed IS scheme, each agent generates an imagined trajectory by modeling the environment dynamics and other agents' actions. The imagined trajectory is the simulated future trajectory of each agent based on the learned model of the environment dynamics and other agents and represents each agent's future action plan. Each agent compresses this imagined trajectory capturing its future action plan to generate its intention message for communication by applying an attention mechanism to learn the relative importance of the components in the imagined trajectory based on the received message from other agents. Numeral results show that the proposed IS scheme outperforms other communication schemes in multi-agent reinforcement learning.", "one-sentence_summary": "This paper propose a new communication scheme named intention sharing to enhance the coordination among agents.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|communication_in_multiagent_reinforcement_learning_intention_sharing", "pdf": "/pdf/8ba121ac29f04d881d760a1f3b9fd0349bb591a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkim2021communication,\ntitle={Communication in Multi-Agent Reinforcement Learning: Intention Sharing},\nauthor={Woojun Kim and Jongeui Park and Youngchul Sung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=qpsl2dR9twy}\n}"}}
{"forum": "9SS69KwomAM", "review_ratings": ["3: Clear rejection", "5: Marginally below acceptance threshold", "6: Marginally above acceptance threshold", "7: Good paper, accept"], "decision": "Poster", "content": {"title": "Solving Compositional Reinforcement Learning Problems via Task Reduction", "authorids": ["~Yunfei_Li1", "wuyilin98@gmail.com", "~Huazhe_Xu1", "~Xiaolong_Wang3", "~Yi_Wu1"], "authors": ["Yunfei Li", "Yilin Wu", "Huazhe Xu", "Xiaolong Wang", "Yi Wu"], "keywords": ["compositional task", "sparse reward", "reinforcement learning", "task reduction", "imitation learning"], "abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.", "one-sentence_summary": "We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|solving_compositional_reinforcement_learning_problems_via_task_reduction", "supplementary_material": "/attachment/54b84ef85ce580bdd6d9d567ff3dabf2d3fcc004.zip", "pdf": "/pdf/77f78b692f36356e5e5bbddd012a3367bd821b29.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021solving,\ntitle={Solving Compositional Reinforcement Learning Problems via Task Reduction},\nauthor={Yunfei Li and Yilin Wu and Huazhe Xu and Xiaolong Wang and Yi Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=9SS69KwomAM}\n}"}}
{"forum": "j1RMMKeP2gR", "review_ratings": ["6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "8: Top 50% of accepted papers, clear accept", "5: Marginally below acceptance threshold"], "decision": "Poster", "content": {"title": "Acting in Delayed Environments with Non-Stationary Markov Policies", "authorids": ["~Esther_Derman1", "~Gal_Dalal2", "~Shie_Mannor2"], "authors": ["Esther Derman", "Gal Dalal", "Shie Mannor"], "keywords": ["reinforcement learning", "delay"], "abstract": "The standard Markov Decision Process (MDP) formulation hinges on the assumption that an action is executed immediately after it was chosen. However, assuming it is often unrealistic and can lead to catastrophic failures in applications such as robotic manipulation, cloud computing, and finance. We introduce a framework for learning and planning in MDPs where the decision-maker commits actions that are executed with a delay of $m$ steps. The brute-force state augmentation baseline where the state is concatenated to the last $m$ committed actions suffers from an exponential complexity in $m$, as we show for policy iteration. We then prove that with execution delay, deterministic Markov policies in the original state-space are sufficient for attaining maximal reward, but need to be non-stationary. As for stationary Markov policies, we show they are sub-optimal in general. Consequently, we devise a non-stationary Q-learning style model-based algorithm that solves delayed execution tasks without resorting to state-augmentation. Experiments on tabular, physical, and Atari domains reveal that it converges quickly to high performance even for substantial delays, while standard approaches that either ignore the delay or rely on state-augmentation struggle or fail due to divergence. The code is available at \\url{https://github.com/galdl/rl_delay_basic.git}.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "derman|acting_in_delayed_environments_with_nonstationary_markov_policies", "one-sentence_summary": "In this paper, we derive theoretical results on execution-delay MDPs, and devise a DQN-based algorithm to empirically tackle this setup.", "supplementary_material": "/attachment/d4631a94fc58706b4257c657196130c560897500.zip", "pdf": "/pdf/da00ad1a9fdf6ed49ff470887d9046490c8058b9.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nderman2021acting,\ntitle={Acting in Delayed Environments with Non-Stationary Markov Policies},\nauthor={Esther Derman and Gal Dalal and Shie Mannor},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=j1RMMKeP2gR}\n}"}}
