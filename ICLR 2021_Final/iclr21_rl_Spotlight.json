{"forum": "UcoXdfrORC", "review_ratings": ["7: Good paper, accept", "7: Good paper, accept", "7: Good paper, accept", "7: Good paper, accept"], "decision": "Spotlight", "content": {"title": "Model-Based Visual Planning with Self-Supervised Functional Distances", "authorids": ["~Stephen_Tian1", "~Suraj_Nair1", "~Frederik_Ebert1", "~Sudeep_Dasari2", "~Benjamin_Eysenbach1", "~Chelsea_Finn1", "~Sergey_Levine1"], "authors": ["Stephen Tian", "Suraj Nair", "Frederik Ebert", "Sudeep Dasari", "Benjamin Eysenbach", "Chelsea Finn", "Sergey Levine"], "keywords": ["planning", "model learning", "distance learning", "reinforcement learning", "robotics"], "abstract": "A generalist robot must be able to complete a variety of tasks in its environment. One appealing way to specify each task is in terms of a goal observation. However, learning goal-reaching policies with reinforcement learning remains a challenging problem, particularly when hand-engineered reward functions are not available. Learned dynamics models are a promising approach for learning about the environment without rewards or task-directed data, but planning to reach goals with such a model requires a notion of functional similarity between observations and goal states. We present a self-supervised method for model-based visual goal reaching, which uses both a visual dynamics model as well as a dynamical distance function learned using model-free reinforcement learning. Our approach learns entirely using offline, unlabeled data, making it practical to scale to large and diverse datasets. In our experiments, we find that our method can successfully learn models that perform a variety of tasks at test-time, moving objects amid distractors with a simulated robotic arm and even learning to open and close a drawer using a real-world robot. In comparisons, we find that this approach substantially outperforms both model-free and model-based prior methods.", "one-sentence_summary": "We combine model-based planning with dynamical distance learning to solve visual goal-reaching tasks, using random, unlabeled, experience.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tian|modelbased_visual_planning_with_selfsupervised_functional_distances", "pdf": "/pdf/e7d9842e2ee26ac0242d6efcf7a863c669541594.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ntian2021modelbased,\ntitle={Model-Based Visual Planning with Self-Supervised Functional Distances},\nauthor={Stephen Tian and Suraj Nair and Frederik Ebert and Sudeep Dasari and Benjamin Eysenbach and Chelsea Finn and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=UcoXdfrORC}\n}"}}
{"forum": "o_V-MjyyGV_", "review_ratings": ["7: Good paper, accept", "7: Good paper, accept", "7: Good paper, accept", "7: Good paper, accept"], "decision": "Spotlight", "content": {"title": "Self-Supervised Policy Adaptation during Deployment", "authorids": ["~Nicklas_Hansen1", "jangirrishabh@gmail.com", "~Yu_Sun1", "~Guillem_Aleny\u00e01", "~Pieter_Abbeel2", "~Alexei_A_Efros1", "~Lerrel_Pinto1", "~Xiaolong_Wang3"], "authors": ["Nicklas Hansen", "Rishabh Jangir", "Yu Sun", "Guillem Aleny\u00e0", "Pieter Abbeel", "Alexei A Efros", "Lerrel Pinto", "Xiaolong Wang"], "keywords": ["reinforcement learning", "robotics", "self-supervised learning", "generalization", "sim2real"], "abstract": "In most real world scenarios, a policy trained by reinforcement learning in one environment needs to be deployed in another, potentially quite different environment. However, generalization across different environments is known to be hard. A natural solution would be to keep training after deployment in the new environment, but this cannot be done if the new environment offers no reward signal. Our work explores the use of self-supervision to allow the policy to continue training after deployment without using any rewards. While previous methods explicitly anticipate changes in the new environment, we assume no prior knowledge of those changes yet still obtain significant improvements. Empirical evaluations are performed on diverse simulation environments from DeepMind Control suite and ViZDoom, as well as real robotic manipulation tasks in  continuously changing environments, taking observations from an uncalibrated camera. Our method improves generalization in 31 out of 36 environments across various tasks and outperforms domain randomization on a majority of environments. Webpage and implementation: https://nicklashansen.github.io/PAD/.", "one-sentence_summary": "Generalization across enviroments is known to be hard. We propose a self-supervised method for policy adaptation during deployment that assumes no prior knowledge of the test environment, yet still obtains significant improvements.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hansen|selfsupervised_policy_adaptation_during_deployment", "supplementary_material": "", "pdf": "/pdf/6949f5e82ffd2bd635a6de802a733540b19b9cc3.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhansen2021selfsupervised,\ntitle={Self-Supervised Policy Adaptation during Deployment},\nauthor={Nicklas Hansen and Rishabh Jangir and Yu Sun and Guillem Aleny{\\`a} and Pieter Abbeel and Alexei A Efros and Lerrel Pinto and Xiaolong Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o_V-MjyyGV_}\n}"}}
{"forum": "30EvkP2aQLD", "review_ratings": ["7: Good paper, accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "8: Top 50% of accepted papers, clear accept"], "decision": "Spotlight", "content": {"title": "What are the Statistical Limits of Offline RL with Linear Function Approximation?", "authorids": ["~Ruosong_Wang1", "~Dean_Foster1", "~Sham_M._Kakade1"], "authors": ["Ruosong Wang", "Dean Foster", "Sham M. Kakade"], "keywords": ["batch reinforcement learning", "function approximation", "lower bound", "representation"], "abstract": "Offline reinforcement learning seeks to utilize offline (observational) data to guide the learning of (causal) sequential decision making strategies. The hope is that offline reinforcement learning coupled with function approximation methods (to deal with the curse of dimensionality) can provide a means to help alleviate the excessive sample complexity burden in modern sequential decision making problems. However, the extent to which this broader approach can be effective is not well understood, where the literature largely consists of sufficient conditions.\n\nThis work focuses on the basic question of what are necessary representational and distributional conditions that permit provable sample-efficient offline reinforcement learning. Perhaps surprisingly, our main result shows that even if: i) we have realizability in that the true value function of \\emph{every} policy is linear in a given set of features and 2) our off-policy data has good  coverage over all features (under a strong spectral condition), any algorithm still (information-theoretically) requires a number of offline samples that is exponential in the problem horizon to non-trivially estimate the value of \\emph{any} given policy. Our results highlight that sample-efficient offline policy evaluation is not possible unless significantly stronger conditions hold; such conditions include either having low distribution shift (where the offline data distribution is close to the distribution of the policy to be evaluated) or significantly stronger representational conditions (beyond realizability).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|what_are_the_statistical_limits_of_offline_rl_with_linear_function_approximation", "one-sentence_summary": "Exponential lower bounds for batch RL with linear function approximation. ", "pdf": "/pdf/abc8c212aff033c6042f681a3e27a3c9af250066.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwang2021what,\ntitle={What are the Statistical Limits of Offline {RL} with Linear Function Approximation?},\nauthor={Ruosong Wang and Dean Foster and Sham M. Kakade},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=30EvkP2aQLD}\n}"}}
{"forum": "HgLO8yalfwc", "review_ratings": ["7: Good paper, accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "6: Marginally above acceptance threshold", "8: Top 50% of accepted papers, clear accept"], "decision": "Spotlight", "content": {"title": "Regularized Inverse Reinforcement Learning", "authorids": ["~Wonseok_Jeon1", "~Chen-Yang_Su1", "~Paul_Barde1", "~Thang_Doan1", "~Derek_Nowrouzezahrai1", "~Joelle_Pineau1"], "authors": ["Wonseok Jeon", "Chen-Yang Su", "Paul Barde", "Thang Doan", "Derek Nowrouzezahrai", "Joelle Pineau"], "keywords": ["inverse reinforcement learning", "reward learning", "regularized markov decision processes", "reinforcement learning"], "abstract": "Inverse Reinforcement Learning (IRL) aims to facilitate a learner\u2019s ability to imitate expert behavior by acquiring reward functions that explain the expert\u2019s decisions. Regularized IRLapplies strongly convex regularizers to the learner\u2019s policy in order to avoid the expert\u2019s behavior being rationalized by arbitrary constant rewards, also known as degenerate solutions. We propose tractable solutions, and practical methods to obtain them, for regularized IRL. Current methods are restricted to the maximum-entropy IRL framework, limiting them to Shannon-entropy regularizers, as well as proposing solutions that are intractable in practice.  We present theoretical backing for our proposed IRL method\u2019s applicability to both discrete and continuous controls, empirically validating our performance on a variety of tasks.", "one-sentence_summary": "We propose tractable solutions of regularized IRL and algorithms to acquire those solutions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "jeon|regularized_inverse_reinforcement_learning", "supplementary_material": "", "pdf": "/pdf/fdd6861c0f1a3c729dc7a2d59344633293d0df3e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\njeon2021regularized,\ntitle={Regularized Inverse Reinforcement Learning},\nauthor={Wonseok Jeon and Chen-Yang Su and Paul Barde and Thang Doan and Derek Nowrouzezahrai and Joelle Pineau},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=HgLO8yalfwc}\n}"}}
{"forum": "xvxPuCkCNPO", "review_ratings": ["7: Good paper, accept", "7: Good paper, accept", "8: Top 50% of accepted papers, clear accept", "8: Top 50% of accepted papers, clear accept"], "decision": "Spotlight", "content": {"title": "Correcting experience replay for multi-agent communication", "authorids": ["~Sanjeevan_Ahilan1", "~Peter_Dayan1"], "authors": ["Sanjeevan Ahilan", "Peter Dayan"], "keywords": ["multi-agent reinforcement learning", "experience replay", "communication", "relabelling"], "abstract": "We consider the problem of learning to communicate using multi-agent reinforcement learning (MARL). A common approach is to learn off-policy, using data sampled from a replay buffer. However, messages received in the past may not accurately reflect the current communication policy of each agent, and this complicates learning. We therefore introduce a 'communication correction' which accounts for the non-stationarity of observed communication induced by multi-agent learning. It works by relabelling the received message to make it likely under the communicator's current policy, and thus be a better reflection of the receiver's current environment. To account for cases in which agents are both senders and receivers, we introduce an ordered relabelling scheme. Our correction is computationally efficient and can be integrated with a range of off-policy algorithms. We find in our experiments that it substantially improves the ability of communicating MARL systems to learn across a variety of cooperative and competitive tasks.", "one-sentence_summary": "We improve multi-agent learning by relabelling past experience to better reflect current communication policies ", "pdf": "/pdf/85eff27bc850ea7f5ee060f1c1d0156c4703f81b.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ahilan|correcting_experience_replay_for_multiagent_communication", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nahilan2021correcting,\ntitle={Correcting experience replay for multi-agent communication},\nauthor={Sanjeevan Ahilan and Peter Dayan},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xvxPuCkCNPO}\n}"}}
{"forum": "LmUJqB1Cz8", "review_ratings": ["7: Good paper, accept", "9: Top 15% of accepted papers, strong accept", "7: Good paper, accept", "7: Good paper, accept"], "decision": "Spotlight", "content": {"title": "Winning the L2RPN Challenge: Power Grid Management via Semi-Markov Afterstate Actor-Critic", "authorids": ["~Deunsol_Yoon1", "~Sunghoon_Hong2", "~Byung-Jun_Lee1", "~Kee-Eung_Kim4"], "authors": ["Deunsol Yoon", "Sunghoon Hong", "Byung-Jun Lee", "Kee-Eung Kim"], "keywords": ["power grid management", "deep reinforcement learning", "graph neural network"], "abstract": "Safe and reliable electricity transmission in power grids is crucial for modern society. It is thus quite natural that there has been a growing interest in the automatic management of power grids, exempli\ufb01ed by the Learning to Run a Power Network Challenge (L2RPN), modeling the problem as a reinforcement learning (RL) task. However, it is highly challenging to manage a real-world scale power grid, mostly due to the massive scale of its state and action space. In this paper, we present an off-policy actor-critic approach that effectively tackles the unique challenges in power grid management by RL, adopting the hierarchical policy together with the afterstate representation. Our agent ranked \ufb01rst in the latest challenge (L2RPN WCCI 2020), being able to avoid disastrous situations while maintaining the highest level of operational ef\ufb01ciency in every test scenarios. This paper provides a formal description of the algorithmic aspect of our approach, as well as further experimental studies on diverse power grids.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yoon|winning_the_l2rpn_challenge_power_grid_management_via_semimarkov_afterstate_actorcritic", "one-sentence_summary": "We  present  an  off-policy  actor-critic  approach  that  effectively tackles  the  unique  challenges  in  power  grid  management  by  reinforcement learning,  adopting  the hierarchical policy together with the afterstate representation. ", "pdf": "/pdf/19ea8e1953e4da5a00beede7da663e04f6717020.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nyoon2021winning,\ntitle={Winning the L2{\\{}RPN{\\}} Challenge: Power Grid Management via Semi-Markov Afterstate Actor-Critic},\nauthor={Deunsol Yoon and Sunghoon Hong and Byung-Jun Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LmUJqB1Cz8}\n}"}}
{"forum": "xppLmXCbOw1", "review_ratings": ["8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "5: Marginally below acceptance threshold", "9: Top 15% of accepted papers, strong accept"], "decision": "Spotlight", "content": {"title": "Self-supervised Visual Reinforcement Learning with Object-centric Representations", "authorids": ["~Andrii_Zadaianchuk1", "~Maximilian_Seitzer1", "~Georg_Martius1"], "authors": ["Andrii Zadaianchuk", "Maximilian Seitzer", "Georg Martius"], "keywords": ["self-supervision", "autonomous learning", "object-centric representations", "visual reinforcement learning"], "abstract": "Autonomous agents need large repertoires of skills to act reasonably on new tasks that they have not seen before. However, acquiring these skills using only a stream of high-dimensional, unstructured, and unlabeled observations is a tricky challenge for any autonomous agent. Previous methods have used variational autoencoders to encode a scene into a low-dimensional vector that can be used as a goal for an agent to discover new skills. Nevertheless, in compositional/multi-object environments it is difficult to disentangle all the factors of variation into such a fixed-length representation of the whole scene. We propose to use object-centric representations as a modular and structured observation space, which is learned with a compositional generative world model.\nWe show that the structure in the representations in combination with goal-conditioned attention policies helps the autonomous agent to discover and learn useful skills. These skills can be further combined to address compositional tasks like the manipulation of several different objects.", "one-sentence_summary": "The combination of object-centric representations and goal-conditioned attention policies helps autonomous agents to learn useful multi-task policies in visual multi-object environments", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zadaianchuk|selfsupervised_visual_reinforcement_learning_with_objectcentric_representations", "pdf": "/pdf/17a02894cb2794484690465818a08e4b35ea6982.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzadaianchuk2021selfsupervised,\ntitle={Self-supervised Visual Reinforcement Learning with Object-centric Representations},\nauthor={Andrii Zadaianchuk and Maximilian Seitzer and Georg Martius},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xppLmXCbOw1}\n}"}}
{"forum": "LwEQnp6CYev", "review_ratings": ["6: Marginally above acceptance threshold", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "7: Good paper, accept"], "decision": "Spotlight", "content": {"title": "Quantifying Differences in Reward Functions", "authorids": ["~Adam_Gleave1", "~Michael_D_Dennis1", "~Shane_Legg1", "~Stuart_Russell1", "~Jan_Leike1"], "authors": ["Adam Gleave", "Michael D Dennis", "Shane Legg", "Stuart Russell", "Jan Leike"], "keywords": ["rl", "irl", "reward learning", "distance", "benchmarks"], "abstract": "For many tasks, the reward function is inaccessible to introspection or too complex to be specified procedurally, and must instead be learned from user data. Prior work has evaluated learned reward functions by evaluating policies optimized for the learned reward. However, this method cannot distinguish between the learned reward function failing to reflect user preferences and the policy optimization process failing to optimize the learned reward. Moreover, this method can only tell us about behavior in the evaluation environment, but the reward may incentivize very different behavior in even a slightly different deployment environment. To address these problems, we introduce the Equivalent-Policy Invariant Comparison (EPIC) distance to quantify the difference between two reward functions directly, without a policy optimization step. We prove EPIC is invariant on an equivalence class of reward functions that always induce the same optimal policy. Furthermore, we find EPIC can be efficiently approximated and is more robust than baselines to the choice of coverage distribution. Finally, we show that EPIC distance bounds the regret of optimal policies even under different transition dynamics, and we confirm empirically that it predicts policy training success. Our source code is available at https://github.com/HumanCompatibleAI/evaluating-rewards.", "one-sentence_summary": "A theoretically principled distance measure on reward functions that is quick to compute and predicts policy training performance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gleave|quantifying_differences_in_reward_functions", "supplementary_material": "", "pdf": "/pdf/c9babbffccc1b8e389a2e8de1c7aac4cee00f966.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngleave2021quantifying,\ntitle={Quantifying Differences in Reward Functions},\nauthor={Adam Gleave and Michael D Dennis and Shane Legg and Stuart Russell and Jan Leike},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LwEQnp6CYev}\n}"}}
